\documentclass[]{final_report}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{titlesec}
\usepackage[utf8]{inputenc}
\usepackage[backend=biber, style=ieee]{biblatex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{tabularx} % Needed for the X column type
\usepackage{booktabs} % For prettier tables
\usepackage{lipsum}   % For dummy text
\usepackage{mathtools}
\usepackage{stmaryrd}
\usepackage{mdframed}
\usepackage{caption}
\usepackage{array}
\usepackage{listings}
\usepackage[dvipsnames]{xcolor}
\usepackage{adjustbox}
\usepackage{tikz}
\usepackage{rotating}
\usepackage{pdflscape}
\usepackage{fancyhdr}
\usepackage{comment}
\usepackage{array}
\usepackage{multirow}
\usepackage{tcolorbox} 
\usepackage{pdflscape}
\usepackage{lscape} % Use lscape package instead of pdflscape
\usepackage{afterpage} % Required for \afterpage command
\usepackage{fancyhdr} % Required for custom headers and footers
\usepackage{everypage}
\usepackage{lipsum}




\addbibresource{fyp.bib}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{basic}{Basic Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{implication}{Implication}

% Define the Java code style
\lstdefinestyle{mystyle}{
    language=Java,
    commentstyle=\color{OliveGreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{gray},
    stringstyle=\color{red},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

% Define the landscape page number command
\newcommand{\Lpagenumber}{%
  \ifdim\textwidth=\linewidth
    \else
      \bgroup
      \dimendef\margin=0 %use \margin instead of \dimen0
      \ifodd\value{page}
        \margin=\oddsidemargin
      \else
        \margin=\evensidemargin
      \fi
      \raisebox{\dimexpr -\topmargin-\headheight-\headsep-0.5\linewidth}[0pt][0pt]{%
        \rlap{\hspace{\dimexpr \margin+\textheight+\footskip}%
        \llap{\rotatebox{90}{\thepage}}}}%
      \egroup
  \fi
}

% Add the page number command to every page
\AddEverypageHook{\Lpagenumber}
\newcommand{\mycomment}[1]{}
%%%%%%%%%%%%%%%%%%%%%%
%%% Input project details
\def\studentname{Jude Asare}
\def\reportyear{2023/2024}
\def\projecttitle{Implementing PKCS\#1 v1.5 and other deterministic RSA Signature Schemes with provably secure parameters}
\def\supervisorname{Saqib Kakvi}
\def\degree{MSci (Hons) in Computer Science (Information Security)}
\def\fullOrHalfUnit{MSci} 
\def\finalOrInterim{Interim Report} 
\begin{document}

\maketitle





%%%%%%%%%%%%%%%%%%%%%%
%%% Table of Contents
\tableofcontents\pdfbookmark[0]{Table of Contents}{toc}\newpage



%%%%%%%%%%%%%%%%%%%%%%
%%% Your Abstract here
\begin{comment}
\mycomment{

\begin{abstract}



The PKCS\#1 v1.5 digital signature scheme has been widely utilised in protocols such as SSH, DNSSEC, IKE, and most prominently, in TLS up to version 1.2. Since its inception in 1998 \cite{rfc2313} it has played a pivotal role in the landscape of digital security comprising the most widely used digital signature scheme in practice. The scheme, renowned for its straightforwardness and expedited verification capabilities, has seen persistent integration across diverse cryptographic systems. 

Nevertheless, amidst its widespread acceptance, it's been marred by several challenges. Among these are the targeting of a latent vulnerability with incorrectly implemented signature verification implementations \cite{finney2006bleichenbacher, kuhn2008variants, bock2018return} stemming from attacks \cite{bleichenbacher1998chosen} exploiting its provably insecure associated encryption paradigm \cite{rfc2313} and a glaring absence of a rigorous security proof that validates its robustness.

Even though alternatives like RSA-PSS offer provable security (\cite{bellare1996exact}, \cite{jonsson2001security}), they come with inherent problem, such as the introduction of randomness and a hike in computational complexity. These issues have created reservations for its wholesale adoption in place of PKCS\#1 v1.5 e.g., RSA-PSS and was only upgraded to a requirement for new applications in PKCS\#1 v2.2 \cite{rfc8017} long after initially being suggested as the replacement for PKCS\#1 v1.5 in PKCS\#1 v2.1 \cite{rfc3447}. Imperative requirements of backward compatibility and interoperability have been the main detractors of the aversion to replacing PKCS\#1 v1.5 and while RSA-PSS is now required in new applications, they entail retaining PKCS\#1 v1.5 in some form at the very least, as a preferable choice.
 
A significant breakthrough came in 2018 when Jager, Kakvi, and May \cite{jager2018security} provided a security proof for the PKCS\#1 v1.5 Signature scheme building on the work of Coron \cite{coron2002security}. Although still requiring the adoption of larger cryptographic parameters deviating slightly from standard use, their methods were flexible enough to show instantiations in practice such that the improved proofs apply. Benefits not limited to PKCS\#1 v1.5, their work also offered insights enabling the proof to apply to other deterministic RSA signature schemes, with similar construction patterns including ISO/IEC 9796-2 and ANSI X9.31 schemes.

Guided by this revelation, this project seeks to concretely implement these signature schemes, with a primary emphasis on the PKCS\#1 v1.5 signature scheme, using the aforementioned provably secure parameters. This project primarily aims to dissect the burden on computational these parameters introduce into deterministic RSA schemes. This aim is supplemented by supporting objectives to produce algorithms that facilitate implementation of the schemes with provably secure and standard parameters. From there the main objective is to produce a user-interfaced benchmarking program to explore aforementioned overhead across various key sizes for the different deterministic signature schemes. 
\newpage
\end{abstract}
}
\end{comment}
\newpage


%%%%%%%%%%%%%%%%%%%%%%
%%% Introduction
\chapter{Introduction}


\section{Aims}

\subsection{Why: The Context and Rationale}
The PKCS\#1 v1.5 digital signature scheme, rooted in RSA's hash-and-sign framework, has emerged as the de facto standard for digital signatures since its 1998 introduction \cite{rfc2313}. Essential in security-focused network protocols like SSH, DNSSEC, IKE, and pre-TLS 1.3 X.509 certificates \cite{schaad2005additional}, its simplicity fostered broad adoption from its outset with a straightforward implementation across programming languages, and speedy verifications relative to alternatives like DSA or ECDS  \cite{jager2018security}. 
Despite its widespread adoption the scheme lacks formal security proofs, raising concerns about its long-term reliability.
Given its deep integration in various applications, even at the hardware level, transitioning to provably secure \cite{bellare1996exact, jonsson2001security} alternatives like RSA-PSS has been slow (RSA-PSS was only upgraded to a requirement for new applications in PKCS\#1 v2.2 \cite{rfc8017} long after initially being suggested as the replacement for PKCS\#1 v1.5 in PKCS\#1 v2.1 \cite{rfc3447}) with imperative requirements of backward compatibility and interoperability acting as significant barriers to full adoption.

Additionally considered in a broader sense, for a cryptographic scheme i.e., RSA-PSS to gain traction it must first be developed and then subjected to rigorous scrutiny by the cryptographic community. Following this, its algorithmic primitives need to be standardised, which then facilitates the creation and standardisation of high-level protocols incorporating these components. Subsequent software implementations must align with these standards before the scheme can be set as the default in various applications. RSA-PSS, while superior to the older PKCS\#1 v1.5 lingers in the early stages of this transition mainly between standardisation of its algorithm primitives and the standardisation of protocols that incorporate its primitives. The current landscape is one of fragmentation where RSA-PSS and PKCS\#1-v1.5 often coexist within the same infrastructure, such as TLS implementations and RSA certificates. This state of the art considered, retaining PKCS\#1 v1.5 scheme remains a preferable choice. Going even further, finding an applicable security proof to bridge its security to the level of its widespread adoption would be the ideal scenario.

In this direction a landmark development came in 2018 when Jager, Kakvi, and May \cite{jager2018security} provided a security proof (an improvement on Coron's proof that was limited to the Rabin-Williams variant with e=2 \cite{coron2002security}) for the PKCS\#1 v1.5 signature scheme, with a caveat: usage of larger parameters are required. Their work offered insights that also apply to other deterministic (and standardised) RSA signature schemes, including ISO/IEC 9796-2 Scheme 1 and ANSI X9.31 rDSA.

\subsection{What: The Primary Goal}
The aim of this project was to connect the theoretical concepts introduced by Jager et al. \cite{jager2018security} with practical application. This involved implementing provably secure parameters and then assessing the burden on computational time that arises when using them in deterministic RSA signature schemes.

\subsection{How: Overview of Objectives}
The project pursued a methodical approach to assess the impact on computational time of employing provably secure parameters in deterministic RSA signature schemes. This was achieved by developing algorithms respective to both cases (standard vs. provably secure parameters) and comparing them for various key sizes when applied to large batches of data (in the case of signature operations) or in the case of key generation, generating keys repeatedly for each parameter type at scale.
The investigation spanned across various signature scheme standards as means of enhancing the reliability and accuracy of the findings by offering views into the computational overhead from within the context of different schemes. The final deliverable constituted an all-incorporative user-interfaced benchmarking program for which results from each of the signature schemes working on batches of data for multiple key sizes were generated.

\subsection{Overview of Results}
Overall, the results indicated that the integration of provably secure parameters into deterministic signature schemes, far from being a computational burden, can rather lead to substantial improvements in verification times (hash function dependent), while there is no observed impact on signature creation and the impact on key generation remains minimal becoming negligible for larger key sizes.



\section{Objectives}
To achieve the aim, the project involved:
\begin{itemize}
    \item Exploring the security proof relevant to the PKCS\#1 v1.5 signature scheme .
    \item Determining the practical implications arising from the security proof on instantiations of the full suite of deterministic signature schemes (PKCS\#1-v1.5, ISO/IEC-9796-2, and ANSI-X9.31 signature schemes).
    \item Developing concrete implementations of the considered deterministic signature schemes using standard and provable parameters.
    \item Demonstrating in practice, with larger parameters, how deterministic schemes can achieve a security level on par with less-practical, signature schemes, such as RSA-PSS.
      \item Evaluating and comparing computational time of instantiating the considered signature schemes with standard vs. provable parameters. 
     \item Creating a user-interfaced benchmarking program that incorporates all objectives of the project to assess the overhead across various key sizes for the different deterministic signature schemes.

\end{itemize}



\begin{comment}
Key Generation results indicated use of provably secure parameters marginally impacts the efficiency of key generation positively at lower key sizes with the effect dissipating with increase in key size. Signature creation results indicated negligible performance between standard and provably secure parameters across all tested schemes. Signature verification results indicated significant improvement in times when using provably secure parameters as compared to standard parameters. This enhancement, however, varied depending on the security level of the hash function. 
\end{comment}

\section{Organisation}
The structure of this paper is outlined as follows: Chapter 2 presents related work on the provable security of deterministic schemes. Chapter 3 covers the background theory for RSA signature schemes. Chapter 4 discusses the proof theorems and draws out their practical implications. Chapter 5 explores assumptions used in implementation, these being derived from the implications presented in Chapter 4. Chapter 6 details the complete development of the project's deliverable. Chapter 7 presents the results, coupled with summaries of key findings. Finally, Chapter 8 concludes the paper with a synthesis of the discussed points.


\chapter{Literature Review/Related Work}
RSASSA-PKCS1-v1.5 remains unbroken. There are no real attacks able to successfully exploit the scheme free of implementation errors. This distinction of being free of implementation errors is crucial. Potential proofs consider only forgeries that are accepted by a correct verification algorithm. It is now well established from a variety of follow up studies all originating from \cite{bleichenbacher1998chosen} that vulnerable implementations of a flawed signature verification algorithm for RSASSA-PKCS1-v1.5 can be exploited. Bleichenbacher presented a low-exponent attack on RSA-PKCS\#1 v1.5 signatures at the CRYPTO 2006 rump session. This attack was later described by Finney \cite{finney2006bleichenbacher} in a posting to the OpenPGP mailing list. 

It was not until the efforts of Coron in 2002 (\cite{coron2002security}) that a security proof applicable to RSASSA-PKCS1-v1.5 arrived. This was due to the issue of deterministic padding scheme that RSASSA-PKCS1-v1.5 uses, rendering standard proof techniques void. Coron presented a security proof for RSASSA-PKCS1-v1.5 (and ISO/IEC 9796-2 signatures) albeit with a restriction that e = 2, i.e. the Rabin-Williams variant \cite{coron2002security} which is secure based on the factoring assumption. 
The proofs' exclusive and/or restrictive value of e aside, a further caveat was that the output size of the hash function needed to be 2/3 the bit length of the modulus N. These restrictions diverge largely from the standard parameters that could potentially be used in the instantiation of RSA-PKCS\#1 v1.5 signatures in practice. 

Much later, Jager, Kakvi and May \cite{jager2018security} showed an improved security proof for RSASSA-PKCS1-v1.5 with less restrictive conditions.
It sufficed that e more generally be a small prime (Kakvi and Kiltz \cite{kakvi2018optimal}). Still requiring a large hash function output, the authors achieved an improvement in hash function output, requiring only 1/2 of the modulus size. The modulus effectively doubles in bit length when compared to the norm with a newly introduced third prime factor necessitating the increase in bits.
Withstanding the improvement and as a consequence of the proofs being founded in the random oracle model, the larger cryptographic parameters still deviate slightly from the standard parameters that could be used in practice. Nonetheless this was sufficient enough for the authors to demonstrate how RSA-PKCS\#1 v1.5 signatures can be instantiated in practice such that the improved proofs apply. 

To all intents and purposes, regardless of the proofs being presented primarily for RSASSA-PKCS1-v1.5, uniformity in construction philosophy means other signature standards of the same deterministic RSA type (ISO/IEC 9796-2 signatures and ANSI X9.31 rDSA) still match the setting required for proofs to be applicable to them. For example theorem statements used in construction of Corons' proof \cite{coron2002security} are general enough that corresponding proof theorems can also be presented for the remaining standards of deterministic signatures. 

A full discussion of provable security extending to non-deterministic schemes lies beyond the scope of this project. Probabilistic padding poses an issue since generating sources of randomness, particularly on constrained devices is an ongoing challenge. Moreover RSA-PSS, the sole RSA-based randomised digital signature scheme uses two hash functions. This makes it difficult to compare with deterministic schemes that use only one. This project focused on standardised deterministic schemes which are directly comparable and subversion resistant \cite{ateniese2015subversion} by default of not having to generate randomness. This lends well to the issue of examining computational overhead from various perspectives of the different deterministic standards.



\chapter{Cryptographic Foundations}
\section{Notations}
The security parameter is denoted as \( \lambda \). 
This parameter determines a system's security level or key sizes. A larger value of $\lambda$ indicates stronger security, though it requires more computational effort. The concept of binary strings is frequently referred to. Specifically for all \( n \in \mathbb{N} \), the symbol \( 1^n \) represents the n-bit string of consecutive ones. 

In cryptographic operations, sampling often involves randomness. Given any set \( S \), the notation \( x \in_{R} S \) signifies that \( x \) is chosen uniformly at random from \( S \).  
The set of prime numbers is represented as \( \mathbb{P} \) and the set of \( k \)-bit primes is denoted as \( \mathbb{P}[k] \). Similarly, the set of integers is represented by \( \mathbb{Z} \) and \( \mathbb{Z}[k] \) respectively. 

The notation \( \mathbb{Z}_N^* \) represents the multiplicative group modulo \( N \) where \( N \in \mathbb{N} \). 

Finally, game-based proofs are employed, and the notation \( G^A \Rightarrow 1 \) indicates an event where the adversary \( A \) succeeds in game \( G \), specifically when the Finalise Procedure yields an output of 1.


\section{Digital Signature Schemes}
In the realm of security, not every issue revolves around confidentiality. In many instances, the adversaries are not limited to passive surveillance but can be active opponents capable of manipulating or introducing unauthorised messages into a communication stream. In the public key setting, the cryptographic primitive used to provide not only data integrity but also for ensuring the authenticity and non-repudiation of communications is a digital signature scheme. 
Essentially, Digital signatures function as a means of binding an identity to specific information. A signature process consists of transforming the relevant message and the entity’s confidential details to produce tag named a signature. This process serves as a verifiable stamp of authenticity, confirming that the message genuinely originated from the claimed source. Moreover it addresses the concept of non-repudiation. Once a message is signed, the signer is bound to the act of signing and cannot deny having signed a then sent message.

An everyday application of digital signatures is in the domain of web security, specifically in the use of Public Key Infrastructure (PKI) and Certificate Authorities (CAs) for verifying the authenticity of websites. For example, when a user visits a website, the site's digital certificate, issued by a CA, is used to establish a secure connection. The digital signature on the certificate verifies that the certificate is indeed issued by a legitimate Certificate Authority and has not been tampered with. This ensures that the website the user is connecting to is actually the one it claims to be, and not a fraudulent site attempting to impersonate it. 

\begin{definition}
\label{def:digital signature}
A (digital) signature scheme DS consists of three probabilistic polynomial-time algorithms (Gen, Sign, Verify) defined as follows:
\begin{enumerate}
    \item Gen (key-generation algorithm): takes as input the security parameter $1^\lambda$ and outputs a pair of keys $(sk ,vk)$, where $sk$ is the signing key and $vk$ is the verification key.
    \item Sign (signing algorithm): takes as input a private key $sk$, message $m$ and outputs a signature $\sigma$ ($\sigma \leftarrow \text{Sign}_{sk}(m)$).
    \item Verify (deterministic verification algorithm):  takes as input a public key $pk$, message $m$, signature $\sigma$ and outputs a boolean ($b := \text{Vrfy}_{pk}(m, \sigma)$).
\end{enumerate}
\end{definition}

The correctness of DS can be confirmed if for any $\lambda \in \mathbb{N}$ and all (pk, sk) output by Gen($1^\lambda$), it holds that: 
\[ Pr[\text{Verify}(\text{pk, m, Sign}(sk, m)) = 1]\]
 
Such a signature scheme can be utilised in the following way.
Bob, the sender, initiates the process by running Gen($1^\lambda$), which generates a pair of keys, specifically the private key (sk) and the public key (vk). He then discloses his public key (vk) to all, including Alice, the intended receiver.

When the need arises for Bob to send a message m to Alice, he creates a signature $\sigma$ using his private key: $\sigma \leftarrow \text{Sign}_{sk}(m)$. This signature, along with the message itself, (m, $\sigma$), is then dispatched to Alice.

Upon receiving the message and its corresponding signature (m, $\sigma$), Alice, already in possession of Bob's public key, proceeds to verify the authenticity of the message. This is done by checking whether $\text{Vrfy}_{vk}(m, \sigma) \stackrel{?}{=} 1$. This process assures Alice of the discussed properties on the message and/or its initiator Bob.

\section{Security of signature schemes}
Security for a signature scheme, represented as DS = (Gen, Sign, Vrfy), is depicted through a contest between a challenger and an Adversary A (probabilistic machine operating within polynomial time). This contest emulates a situation in which A endeavours, to compromise the signature scheme by employing a specific attack model. 

\subsection{Default notion of Secure Signatures}
The intuitive idea behind the default notion of security for digital signature schemes is that no efficient adversary should be able to generate valid digital signature for any "new" document that was not previously signed by the original signer.
However, adversaries may gain access to all documents and their corresponding signatures through a sign oracle, and they may also exert influence over the content of documents, thus enabling replay attacks.

A robust digital signature system, resistant to such forgeries, is termed "existentially unforgeable under an adaptive chosen-message attack". "Existentially unforgeable" signifies that the adversary cannot produce a valid signature on any document. The protection  remains intact even if the adversary can carry out an adaptive chosen-message attack (hence the latter phrasing) by which it is able to obtain signatures on arbitrary messages chosen adaptively during its attack.

\begin{figure}[H]
\centering
\hfill Game UF-CMA(ROM)\hfill\phantom{} 
\[
\boxed{
\begin{aligned}
&\underline{\textbf{\text{Initialise}}} \\
&(pk, sk) \xleftarrow{\$} \text{Gen}(1^\lambda) \\
&\text{return } pk \\
\\
&\underline{\textbf{\text{Hash}}(m)} \\
&\text{if } (m, \cdot) \in \mathcal{H} \\
&\quad \text{fetch } (m, y) \in \mathcal{H} \\
&\quad \text{return } y \\
&\text{else} \\
&\quad y \in_{R} \text{Domain}; \\
&\quad \mathcal{H} \leftarrow \mathcal{H} \cup\{(m, y)\} \\
&\quad \text{return } y \\
\\
&\underline{\textbf{\text{Sign}}(m)} \\
&\mathcal{M} \leftarrow \mathcal{M} \cup\{m\} \\
&\text{return } \sigma \xleftarrow{\$} \text{Sign}(sk, m) \\
\\
&\underline{\textbf{\text{Finalise}}(m^*, \sigma^*)} \\
&\text{if Vrfy}(pk, m^*, \sigma^*) == 1 \land m^* \notin \mathcal{M} \\
&\quad \text{return } 1 \\
&\text{else} \\
&\quad \text{return } 0
\end{aligned}
}
\]
\caption{Game defining UF-CMA security in the Random Oracle Model (section \ref{subSec:ROM}.)}
\label{fig:crypto_game}
\end{figure}
\begin{definition}
A signature scheme DS is UF-CMA secure, if for any forger $\mathcal{F}$ running in time at most $t$, making at most $q_h$ hash queries and making at most $q_s$ signature queries, we have:
\[
\textbf{Adv}_{\mathcal{F},\text{DS}}^{\text{UF-CMA}} = \Pr \left[ \begin{aligned} &1 \leftarrow \textbf{Finalise}(m^*, \sigma^*); \\ &(m^*, \sigma^*) \leftarrow \mathcal{F}^{\textbf{Hash}(\cdot),\textbf{Sign}(\cdot)}(pk); \\ &\texttt{pk} \xleftarrow{\$} \textbf{Initialise}(1^{\lambda}) \end{aligned} \right] \leq \varepsilon
\]
\end{definition}

%\text{pk} \leftarrow_{\$} \text{Initialise}(1^{\lambda})
\subsection{Stronger Notion of Secure Signatures}
While a secure digital signature ensures that an adversary cannot forge a signature for a new, previously unsigned message, it does not prevent the adversary from generating a new, valid signature for a message that has already been signed. To address this, a stronger notion of security is needed.

Consider a modified game \texttt{SUF-CMA(ROM)} defined in exactly the same way as Game \texttt{UF-CMA(ROM)}, except that  now queries are to be restricted to an underlying Signature-Messages space $\mathcal{S}$. $\mathcal{S}$ instead contains pairs of oracle queries and their associated responses (e.g., $(m^*, \sigma^*) \in \mathcal{S}$ if  $\mathcal{F}$ queried Sign$(m^*)$ and received in response the signature $\sigma^*$). The Sign and Finalise oracles are adapted as follows:
\begin{figure}[H]
\centering
\hfill Game SUF-CMA(ROM)\hfill\phantom{} 
\[
\boxed{
\begin{aligned}
&\underline{\textbf{\text{Sign}}(m)} \\
&\sigma \xleftarrow{\$} \text{Sign}(sk, m) \\
&\mathcal{S} \leftarrow \mathcal{S} \cup\{(m, \sigma)\} \\
&\text{return } \sigma \\
\\
&\underline{\textbf{\text{Finalise}}(m^*, \sigma^*)} \\
&\text{if Vrfy}(pk, m^*, \sigma^*) == 1 \land (m^*, \sigma^*) \notin \mathcal{S} \\
&\quad \text{return } 1 \\
&\text{else} \\
&\quad \text{return } 0
\end{aligned}
}
\]
\caption{Game defining SUF-CMA}
\label{fig:crypto_game}
\end{figure} 
\begin{definition}
A signature scheme DS is said to be \emph{Strong Existentially Unforgeable under an Adaptive Chosen-Message Attack} (SUF-CMA) secure if, for any forger \(\mathcal{F}\) running in time at most \(t\), making at most \(q_h\) hash queries and making at most \(q_s\) signature queries, we have:
\[
\textbf{Adv}_{\mathcal{F},\text{DS}}^{\text{SUF-CMA}} = \Pr \left[ \begin{aligned} &1 \leftarrow \textbf{Finalise}(m^*, \sigma^*); \\ &(m^*, \sigma^*) \leftarrow \mathcal{F}^{\textbf{Hash}(\cdot),\textbf{Sign}(\cdot)}(pk); \\ &\texttt{pk} \xleftarrow{\$} \textbf{Initialise}(1^{\lambda}) \end{aligned} \right] \leq \varepsilon
\]
\end{definition}



\section{RSA}
RSA public key algorithm was developed by Ron Rivest, Adi Shamir and Leonard Adleman in 1977 \cite{10.1145/359340.359342}. Soon it became popular and was adopted by many standard bodies.
\subsection{RSA Key Generation}
\label{subSec:keygen}
RSA is widely used as the basis for digital signature schemes. There are various methods for generating digital signatures using RSA functions based on the RSA assumption. 

While these methods differ in terms of operations in relation to signature and/or verification algorithms, the RSA key generation procedure is common to all RSA-based signature schemes.  An RSA key consists of three elements: A modulus N, a public exponent e and a private exponent d. The pair (N, e) constitutes the public key while (N, d) forms the private key.

\begin{definition}
Let \textit{GenModulus} be a polynomial-time algorithm that, that on inputs ($1^\lambda$, k, ($\lambda_{1},...,\lambda_{k}$) outputs (N, ($p_{1},...,p_{k}$)) where $N = \displaystyle\prod_{i=1}^{k} p_{i}$ i.e., the product of k distinct random prime numbers $p_i \in_{R} \mathbb{P}[\lambda_i]$, for $i \in \llbracket1, \ldots, k \rrbracket$ for k constant. 
\end{definition}

To generate an RSA key pair, Each entity B runs the following algorithm:

\begin{definition} GenRSA
\label{def:GenRSA}
\begin{mdframed}
\begin{enumerate}
    \item Run GenModulus $(N, p_{1},...,p_{k}) \leftarrow GenModulus(1^\lambda, k, (\lambda_{1},...,\lambda_{k}))$
    \item Compute $\phi(N) = \displaystyle\prod_{i=1}^{k} (p_i - 1)$
    \item Select an arbitrary integer e, $e > 1$, such that gcd(e, $\phi(N)$) = 1 
    \item Compute d, $1 > d < \phi(N)$, such that $ed \equiv 1 \text{ } (mod \text{ } \phi(N))$
    \item return N, e, d
\end{enumerate}
\end{mdframed}
\end{definition}

The exponents are chosen in a way that for any number S with $S < N$, the following is always true:
\[S = M^{d \cdot e} \bmod N = M^{e \cdot d} \bmod N \text{ } (1)\]

\subsection{RSA Assumption}
\label{subSec:RSA-ASS}
RSA's security is closely tied into the hardness of factoring with the concept of the RSA problem. Notably It's widely believed that if one could efficiently factor N into its prime components, they could solve the RSA problem. While the converse is not proven (if one could solve the RSA problem, this does not equate to factorising N efficiently), the link is strong enough - at least until the use of quantum computers becomes feasible.

Given a modulus \(N\) and a co-prime integer e, exponentiation to the \(e\)th power modulo \(N\) generates a permutation, leading to the RSA problem's core concept. For any number \(y \in \mathbb{Z}^*_{N}\), if \(x^e = y \mod N\), then \(x\) is uniquely defined, setting up the RSA problem to compute \(x = [y^{1/e} \mod N]\), or the \(e\)th root modulo \(N\), without the factorisation of \(N\). Informally this is the RSA assumption.

\textbf{Trapdoors and RSA}. The strength of the RSA algorithm can be seen through the lens of trapdoor permutations. This process is easy to compute in one direction (finding \(y\)) but computationally hard in reverse (finding \(x\)), without the trapdoor \(d\), which allows for the easy computation of the eth root modulo N.

\begin{definition}
\label{def:RSA-ASS}
(RSA Assumption \cite{10.1145/359340.359342}). The k-\texttt{RSA}[$\lambda$], states that given $(N, e, x^e)$ it is hard to compute $x$, 

where $N$ is a $\lambda$-bit number such that $N = \displaystyle\prod_{i=1}^{k} p_i$ 

i.e., the product of k distinct random prime numbers $p_i \in_{R} \mathbb{P}[\lambda_i]$, for $i \in \llbracket1, \ldots, k \rrbracket$, for k constant. 

Additionally, $e \in \mathbb{Z}_{\phi(N)}^{*}$, and $x \in_R \mathbb{Z}_N$. k-\texttt{RSA}[$\lambda$] is said to be ($t, \varepsilon$)-hard, if for all adversaries $ \mathcal{A}$ running in time at most $t$, we have

\[ \textbf{Adv}_ \mathcal{A}^{\text{k-}\texttt{RSA}[\lambda]} = \Pr [x =  \mathcal{A}(N, e, x^e \bmod N) ] \leq \varepsilon. \]
\end{definition}

The assumption is equivalent to viewing of the RSA function as a trapdoor function. In turn it can be said that the security of any RSA-based signature scheme can be reduced to the security of the RSA function as a trapdoor function.


\section{Textbook RSA}
In its most basic form, RSA offers a clear blueprint for digital signatures. The loose resemblance between signatures and the RSA function hinges on their shared trait of asymmetry. While everyone should have the ability to verify a signature only the one possessing the signing key can have the capability to create a (legitimate) signature. The RSA function mirrors this asymmetry: If N and e are made public, then anyone can exponentiate using e ($m \stackrel{?}{=} [\sigma^e \bmod N]$), but only the individual with d can exponentiate using d ($\sigma := [m^d \bmod N]$). 

Regrettably, textbook RSA signatures have security issues because the difficulty of RSA problem does not meaningfully relate to the computation of a signature, especially for non-uniform messages. Adversaries might bypass the RSA problem or deduce new signatures from others, leading to vulnerability.


\begin{figure}[H]
\caption{Multiplicative property}
\begin{equation}
(m_{1}m_{2})^{d} \equiv m_{1}^{d}m_{2}^{d} \bmod N
\end{equation}

\end{figure}
\textbf{Multiplicative attacks} leverage RSA’s inherent mathematical property outlined more generally above: the product of two signatures is a valid signature for the product of their respective messages. More specifically, if \( \sigma_{1} \) and \( \sigma_{2} \) are respective signatures for \( m_{1} \) and \( m_{2} \), then \( \sigma_{1} \cdot \sigma_{2} \) is a valid signature for \( m_{1} \cdot m_{2} \). This allows forging by choosing messages to yield a desired product modulo \( N \) in the most severe form of attack.

\begin{comment}
\textbf{A no message attack}. This attack constitutes a less severe type of forgery that can be performed relying solely on the public key with absence of the signer. By its approach, the adversary can randomly choose a signature \( \sigma \in \mathbb{Z}^*_{N} \) and compute the corresponding message as  \( m \equiv \sigma^{e} \bmod N \). By construction the attack does not enable the adversary direct control over m. However, this may be sufficient in some applications, e.g.,  signing random numbers when issuing some tokens. Additionally it can be possible to influence certain bits or characteristics of m by carefully choosing differing $\sigma$ values over multiple iterations. With a high probability it is then possible to obtain an m with bits set in some desired way.
\end{comment}


\section{Hash-and-Sign}
\label{def:hashed rsa}
Hash-and-Sign is designed to counteract the vulnerabilities observed in the textbook RSA signature schemes by enabling the disruption of algebraic relationships between plaintexts and ciphertexts. Secondly, it accounts for the fact that in real-world applications, messages are often bit strings of arbitrary lengths, not readily compatible with the format required for plain RSA signatures, which typically demand group elements.

To address these issues, a pre-processing step is commonly employed, which involves applying a transformation to a message x i.e., computing y := H(x) where H is a public hash function that outputs elements belonging to $\mathbb{Z}^{*}_{N}$.

Signatures are then computed on the resulting message representative (e.g., $\sigma := [y^d \bmod N]$) while verification now ensures signature equivalence with the corresponding representative (e.g., $\sigma^e \stackrel{?}{=} y \bmod N$). This significantly thwarts the efficacy of the discussed attacks if \( H \) is not efficiently invertible.

\begin{comment}
\begin{definition}
\label{def:hashed rsa}
Let GenRSA be as in the text. The Basic hashed RSA signature scheme.
\begin{mdframed}
\begin{enumerate}
    \item Gen: on input $1^n$ run GenRSA($1^n$) to obtain (N, e, d). The public key is $\langle N, e \rangle$ and the private key is $\langle N, d \rangle$.
    \begin{itemize}
    \item As part of key generation, a function H : $\{0, 1\}^* \rightarrow \mathbb{Z}^*_{N}$ is specified.
    \end{itemize}

    \item Sign: on input of private key sk = $\langle N, d \rangle$ and a message m $\in \{0, 1\}^*$, compute the signature
\[\sigma := [H(m)^d \bmod N].\]
    \item Vrfy (deterministic verification algorithm):  on input of public key pk = $\langle N, e \rangle$, a message m, and a signature $\sigma$, output 1 if and only if
    \[\sigma^e \stackrel{?}{=} H(m) \bmod N.\]
\end{enumerate}
\end{mdframed}

\end{definition}
\end{comment}

\section{Classification of Digital Signature Schemes}
Digital Signature schemes can be divided according to two general classes:
\begin{enumerate}
    \item[] \begin{basic} 
\textit{Digital signature schemes with appendix}. Digital signature schemes where the signature is appended to the message as a separate entity and subsequently both are required for verification. On receipt of the message and signature, the verifier uses the signer's public key to apply the relevant cryptographic operation to the signature, and compares the result to the hash of the message.
\end{basic}
    \item[] \begin{basic}
\textit{Digital signature schemes with message recovery}. Digital signature schemes where a portion of the message (full or partial) is embedded within the signature itself and the verification algorithm only requires a message portion if the full message was unable to be embedded in the signature. In cases of the latter, the portion of the message not embedded is transmitted along with the signature. On receipt of the signature and potentially a message (if applicable), the verifier uses the signer's public key to apply the relevant cryptographic operation to the signature, to recover the padded message and thereafter the message.
\end{basic}

\end{enumerate}

Digital signature schemes with message recovery are useful in scenarios with limited bandwidth or storage capacity, as they reduce the overall data size that needs to be transmitted and stored– for example smart cards. However, In the vast majority of applications this is  unnecessary and signature schemes with appendix are typically the default choice.
\section{RSASSA-PKCS1-v1.5}
The PKCS\#1 signature scheme is one of the earliest standardised Hash-and-Sign signature schemes on record. The scheme was initially disclosed in version 1.5 \cite{rfc2313}, which is why it is typically known as PKCS1 v1.5.

Messages are encoded in representative “blocks” in PKCS v1.5\#1 with the (hexadecimal) format
\[0x00\||BT\|PS\|0x00\|D\]

This format starts with a leading 0x00 block, which serves to guarantee that the entire message representative, when converted to an integer, remains smaller than the modulus N. BT refers to the type of block which is 0x01 for signatures. 'D' represents the encoded message, consisting of the message's hash preceded by the hash identifier $ID_{H}$. 'PS' is the padding string, which is utilised to make sure that the message representative's length matches the bit length of the modulus and consists of a sequence of 0xFF bytes repeated as needed. Finally a trailing 0x00 block in the context of the padding serves as a separator from the encoded message 'D' that follows.
\[0x00\|0x01\|0xFF . . . FF\|0x00\|ID_{H}\|H(m)\]

For all of the signature scheme definitions to follow: let GenRSA be adapted as to not compute and subsequently return d but instead return the prime factors of N as additional arguments.
\begin{definition}
\begin{figure}[H]
\centering
\hfill RSA-PKCS1-v1.5 signatures\hfill\phantom{} 
\[
\boxed{
\begin{aligned}
&\underline{\textbf{\text{Gen}} (1^\lambda, k, (\lambda_{1},...,\lambda_{k}), \ell)} \\
&\text{Run GenRSA}(1^\lambda, k, (\lambda_{1},...,\lambda_{k})) \text{ to obtain } (N, e, (p_{1},...,p_{k})) \\
%&\text{where } \\
%&\quad P, Q \in_{R} \mathbb{P}[\lambda/2] \\
%&\quad\text{2. }p_{3},...,p_{k} \in_{R} \mathbb{P}[\lambda/2]  \\
&\text{Choose a hash function } H : \{0, 1\}^* \rightarrow \{0, 1\}^\ell. \\
&\text{Look-up } \alpha\text{-bit } ID_{H} \text{ for } H \\
&\text{Compute } \nu = n - \ell - \alpha - 23 \\
&\text{Compute PAD} = 0^{15} \| 1^{\nu} \| 0^8 \| ID_{H} \\
&\text{return } (pk = (N, e, PAD, H), sk =  (p_{1},...,p_{k})) \\
\\
&\underline{\textbf{\text{Sign}} (sk, m)} \\
&\text{Compute } z \leftarrow H(m) \\
&\text{Compute } y = PAD  \| z \\
&\text{return } \sigma = y^{1/e} \bmod N \\
\\
&\underline{\textbf{\text{Vrfy}} (pk, m, \sigma)} \\
&\text{Compute } y^{'} = \sigma^{e} \bmod N \\
&\text{Compute } z \leftarrow H(m) \\
&\text{If } (PAD \| z == y^{'}) \\
&\quad \text{return } 1 \\
&\text{else} \\
&\quad \text{return } 0 
\end{aligned}
}
\]
\caption{RSA PKCS\#1 v1.5}
\label{fig:pkcs}
\end{figure}
\end{definition}

A java implementation of the schemes encoding is as follows: 
\begin{lstlisting}[caption=]
  @Override
  protected byte[] encodeMessage(byte[] M) throws DataFormatException {

    this.md.update(M);
    byte[] mHash = this.md.digest();
    byte[] digestInfo = createDigestInfo(mHash);
    int tLen = digestInfo.length;
    
    //Prepare padding string PS consisting of padding bytes (0xFF).
    int psLength =
        this.emLen - tLen - 3;
    byte[] PS = new byte[psLength];
    Arrays.fill(PS, (byte) 0xFF);

    // Concatenate PS, D, and other padding to form the encoded message EM.
    byte[] EM = new byte[emLen];
    int offset = 0;
    EM[offset++] = 0x00; 
    EM[offset++] = 0x01; // Block type 0x01 for PKCS signatures
    System.arraycopy(PS, 0, EM, offset, psLength); // Padding
    offset += psLength;
    EM[offset++] = 0x00; // Separator
    System.arraycopy(digestInfo, 0, EM, offset, tLen); // D

    return EM;
  }
\end{lstlisting}
The full class can be found at application/modules/signatures/models/schemes/RSASSA\_PKCS1\_v1\_5.java.

\section{ANSI X9.31 rDSA Signatures}
ANSI X9.31 \cite{ANSI-1998-X9-31} is another Hash-and-Sign signature scheme standardised around a similar time to RSASSA-PKCS. Designed for use in the banking sector, the scheme is very similar to RSASSA-PKCS1-v1.5. Both variants of the signature schemes with appendix class of signatures, the two differ slightly with respect to padding and ordering of hash related data.


\[PS\|D\]

The format ends with a segment labeled 'D' representing the encoded message, which again includes a hash identifier and the message hash. Notably, the sequence here is inverted from the PKCS format: the message's hash is placed before the hash identifier. Before 'D', there's a padding string (PS) which is set to the sequence 0x6B...BA. Within this, 0xB...B represents the repetitive part of padding, which fills up the necessary space to ensure that the message representative's length matches the length of the modulus. An initial 0x6 nibble signifies the start of padding while a trailing 0x0A nibble indicates the end of padding.
\[0x6B . . . BA\|H(m)\|ID_{H}\]

It should be noted that the considered changes to padding are insignificant in the context of the proofs to follow, as they are both for arbitrary padding.


\begin{definition}
\begin{figure}[H]
\centering
\hfill ANSI X9.31 rDSA signature scheme\hfill\phantom{} 
\[
\boxed{
\begin{aligned}
&\underline{\textbf{\text{Gen}} (1^\lambda, k, (\lambda_{1},...,\lambda_{k}), \ell)} \\
&\text{Run GenRSA}(1^\lambda, k, (\lambda_{1},...,\lambda_{k})) \text{ to obtain } (N, e,  (p_{1},...,p_{k})) \\
%&\text{where } \\
%&\quad P, Q \in_{R} \mathbb{P}[\lambda/2] \\
%&\quad\text{2. }p_{3},...,p_{k} \in_{R} \mathbb{P}[\lambda/2]  \\
&\text{Choose a hash function } H : \{0, 1\}^* \rightarrow \{0, 1\}^\ell. \\
&\text{Look-up } 16\text{-bit } ID_{H} \text{ for } H \\
&\text{Compute } \nu = \frac{(\lambda - \ell - 16 - 8)}{4} \\
&\text{Compute PAD} = 0110 \| (1011)^{\nu} \| 1010 \\
&\text{return } (pk = (N, e, PAD, ID_{H}, H), sk =  (p_{1},...,p_{k})) \\
\\
&\underline{\textbf{\text{Sign}} (sk, m)} \\
&\text{Compute } z \leftarrow H(m) \\
&\text{Compute } y = PAD  \| z \| ID_{H} \\
&\text{return } \sigma = y^{1/e} \bmod N \\
\\
&\underline{\textbf{\text{Vrfy}} (pk, m, \sigma)} \\
&\text{Compute } y^{'} = \sigma^{e} \bmod N \\
&\text{Compute } z \leftarrow H(m) \\
&\text{If } (PAD \| z == y^{'}) \\
&\quad \text{return } 1 \\
&\text{else} \\
&\quad \text{return } 0 
\end{aligned}
}
\]
\caption{ANSI X9.31 rDSA}
\label{fig:ansix931}
\end{figure}
\end{definition}

A java implementation of the schemes encoding is as follows: 
\begin{lstlisting}[caption=]
  @Override
  protected byte[] encodeMessage(byte[] M) {
    byte[] EM = new byte[emLen];
    this.md.update(M);
    byte[] mHash = this.md.digest();
    byte[] digestInfo = createDigestInfo(mHash);
    int tLen = digestInfo.length;
    // copying the message hash to comprise the end
    // of encoded message
    int hashStart = emLen - tLen;
    System.arraycopy(digestInfo, 0, EM, hashStart, tLen);
    int delta = hashStart;

    // Pad with Bs to fill the remaining space
    if ((delta - 1) > 0) {
      for (int i = delta - 1; i != 0; i--) {
        EM[i] = (byte) 0xbb;
      }
      EM[delta - 1] ^= (byte) 0x01;
      EM[0] = (byte) 0x6B;
    } else {
      EM[0] = (byte) 0x6A;
    }
    return EM;
  }
\end{lstlisting}
The full class can be found at application/modules/signatures/models//schemes/ANSI\_X9\_31\_RDSA.java.

\section{ISO/IEC 9796-2:2010 Signature Scheme 1}
The ISO/IEC 9796-2:2010 \cite{ISO/2010/9796-2-2010} Signature Scheme is the final standardised scheme to be considered. The scheme has widespread practical significance, primarily in the payments sector where it is used in the EMV (Europay, Mastercard, and Visa) payment system for chip and pin cards. 

A variant of the signature schemes with message recovery class of signatures, ISO/IEC 9796-2:2010 offers a swift departure from the more common practice of signing with appendix. It differs from convention by embedding either the entire or part of a message within the signature. This allows for the recovery of the corresponding message representative upon verification. The approach yields space and message length savings, although it requires more computational effort to extract the message during verification.

\textbf{EMV Protocol:} The consideration of ISO/IEC 9796-2 Signature Scheme 1 is confined to its application within the EMV standard. Previous versions of the schemes general susceptibility to attacks \cite{10.1007/3-540-48405-1_1, coron2016practical} is thus immaterial since such attacks work only on message spaces that exceed the length of the messages space used by the EMV protocol.

The standard offers two modes: full message recovery for sufficiently shorter messages that can be completely embedded in the signature and partial message recovery, which is applicable for extensively ranged messages that exceed the signature's capacity. As part of the latter, any message excess is transmitted along with the signature.

\begin{table}[H]
    \centering
    \caption{ISO/IEC 9796-2 Signature block Format table}
    \begin{tabular}{|c|c|}
    \hline
    Full Message Recovery & $0x4B_{0},...,B_{q}A\|m\|H(m)\|0xBC$ \\
    \hline
    Partial Message Recovery & $0x6B_{0},...,B_{q}A\|m\|H(m)\|0xBC$ \\
    \hline
    \end{tabular}
    \label{tab:sig_block_tab}
\end{table}

The format of the message representative starts with an initial padding pattern to the left of message which can be denoted as $PAD_L$ for illustration purposes. $PAD_L$ starts with a header nibble that distinguishes between the two variants of the scheme with 0x6 for partial message recovery or alternatively 0x4 for full message recovery. The trailer nibble of 0xA is an indicator to the end of $PAD_L$. 

In between $PAD_L$'s header and the trailing 0x0A nibble is the repetitive portion of padding: an optional sequence of Bs, $B_{0},...,B_{q}$ where q is the number of bytes required to fill in the remaining space for the full message representative. This sequence is optional and is utilised only when the recoverable message following $PAD_L$ is shorter than the available space, as seen in the full message recovery scheme. The first nibble in the $B_{q}$ byte i.e., 0xB0 combines with the trailing 0x0A nibble to finalise $PAD_L$.
After $PAD_L$ is the recoverable message portion which is followed by the hash of the complete message. The signatures conclude with a right padding pattern 0xBC denoted as $PAD_R$.

Typically, descriptions of the scheme do not mention the optional Bs and present $PAD_L$ as solely comprising the header and trailer, i.e., $PAD_L = 0x4A$ or $PAD_L = 0x6A$.

*Note: For formal definitions to follow, for any sufficiently long bit string x, MSBs(x, n) denotes the n most significant bits of x.

\begin{definition}
\begin{figure}[H]
\centering
\hfill ISO/IEC 9796-2:2010 Signature Scheme 1 with partial message recovery\hfill\phantom{} 
\[
\boxed{
\begin{aligned}
&\underline{\textbf{\text{Gen}} (1^\lambda, k, (\lambda_{1},...,\lambda_{k}), \ell)} \\
&\text{Run GenRSA}(1^\lambda, k, (\lambda_{1},...,\lambda_{k})) \text{ to obtain } (N, e, (p_{1},...,p_{k})) \\
%&\text{where } \\
%&\quad P, Q \in_{R} \mathbb{P}[\lambda/2] \\
%&\quad\text{2. }p_{3},...,p_{k} \in_{R} \mathbb{P}[\lambda/2]  \\
&\text{Choose a hash function } H : \{0, 1\}^* \rightarrow \{0, 1\}^\ell. \\
&\text{Compute } PAD_{L} = 01101010 \\
&\text{Compute } PAD_{R} = 10111100 \\
&\text{return } (pk = (N, e, PAD_{L}, PAD_{R}, H), sk =  (p_{1},...,p_{k})) \\
\\
&\underline{\textbf{\text{Sign}} (sk, m)} \\
&\text{Compute } z \leftarrow H(m) \\
&\text{Compute } \nu = \lambda - \ell - 16 \\
&\text{Compute } m_{1} = \text{MSBs}(m, v) \text{ distinct from m such that } m = m_{1} m_{2} \\
&\text{Compute } y = PAD_{L} \| m_{1} \| z \| PAD_{R} \\
&\text{Compute } \sigma = y^{1/e} \bmod N \\
&\text{return } (\sigma, m_{2}) \\
\\
&\underline{\textbf{\text{Vrfy}} (pk, m_{2}, \sigma)} \\
&\text{Compute } y^{'} = \sigma^{e} \bmod N \text{ and parse } y^{'} \text{ as: } \\
&\quad y^{'} = PAD_{L} \| m_{1} \| z \| PAD_{R} \\
&\text{If } (H(m_{1} \| m_{2}) == z) \\
&\quad \text{return } 1 \\
&\text{else} \\
&\quad \text{return } 0 
\end{aligned}
}
\]
\caption{ISO/IEC 9796-2 Scheme 1 (PR)}
\label{fig:isoiec9796}
\end{figure}
\end{definition}


\begin{definition}
\begin{figure}[H]
\centering
\hfill ISO/IEC 9796-2:2010 Signature Scheme 1 with full message recovery\hfill\phantom{} 
\[
\boxed{
\begin{aligned}
&\underline{\textbf{\text{Gen}} (1^\lambda, k, (\lambda_{1},...,\lambda_{k}), \ell)} \\
&\text{Run GenRSA}(1^\lambda, k, (\lambda_{1},...,\lambda_{k})) \text{ to obtain } (N, e, (p_{1},...,p_{k})) \\
%&\text{where } \\
%&\quad P, Q \in_{R} \mathbb{P}[\lambda/2] \\
%&\quad\text{2. }p_{3},...,p_{k} \in_{R} \mathbb{P}[\lambda/2]  \\
&\text{Choose a hash function } H : \{0, 1\}^* \rightarrow \{0, 1\}^\ell. \\
&\text{Compute } \nu = \frac{(\lambda - \ell - 8 - 4)}{4} \\
&\text{Compute } PAD_{L} = 0100 \| (1011)^{\nu} \| 1010 \\
&\text{Compute } PAD_{R} = 10111100 \\
&\text{return } (pk = (N, e, PAD_{L}, PAD_{R}, H), sk = (p_{1},...,p_{k})) \\
\\
&\underline{\textbf{\text{Sign}} (sk, m)} \\
&\text{Compute } z \leftarrow H(m) \\
&\text{Compute } y = PAD_{L} \| m \| z \| PAD_{R} \\
&\text{return } \sigma = y^{1/e} \bmod N \\
\\
&\underline{\textbf{\text{Vrfy}} (pk, \sigma)} \\
&\text{Compute } y^{'} = \sigma^{e} \bmod N \text{ and parse } y^{'} \text{ as: } \\
&\quad y^{'} = PAD_{L} \| m \| z \| PAD_{R} \\
&\text{If } (H(m) == z) \\
&\quad \text{return } 1 \\
&\text{else} \\
&\quad \text{return } 0 
\end{aligned}
}
\]
\caption{ISO/IEC 9796-2 Scheme 1 (FR)}
\label{fig:isoiec9796-full}
\end{figure}
\end{definition}

A java implementation of the schemes encoding with the message recovery mode implicitly selected according to the length of chosen message (M) and modulus (emLen) is as follows: 
\begin{lstlisting}[caption=]
   @Override
  public byte[] encodeMessage(byte[] M) throws DataFormatException {
    byte[] EM = new byte[emLen];
    m1Len = M.length;
    int availableSpace = (hashSize + m1Len) * 8 + 8 + 4 - emBits;
    //Partial recovery if message is larger than available space
    // else scheme proceeds with full recovery.
    if (availableSpace > 0) {
      PADLFIRSTNIBBLE = 0x60;
      isFullRecovery = false;
    } else {
      PADLFIRSTNIBBLE = 0x40;
      isFullRecovery = true;
    }

    int hashStart = emLen - hashSize - 1;
    int delta = hashStart;
    //length of the message to be copied is either the availableSpace most significant bits of
    // M or alternatively the full length of the original message if the message is too short
    int messageLength = Math.min(m1Len, m1Len - ((availableSpace + 7) / 8) - 1);
    // m2 comprises the non-recoverable message portion
    m2Len = max(m1Len - messageLength, 0);
    //copying the message
    delta -= messageLength;
    System.arraycopy(M, 0, EM, delta, messageLength);

    // Hash message as normal for standard case, else apply the variable length hash function 
    // to generate either the preset provable hash output (1/2 length of modulus) or any custom output size
    byte[] hashedM = computeHashWithOptionalMasking(M);
    System.arraycopy(hashedM, 0, EM, hashStart, hashSize);

    // Pad with Bs if m_r (m1) is shorter than the available space
    if ((delta - 1) > 0) {
      for (int i = delta - 1; i != 0; i--) {
        EM[i] = (byte) 0xbb;
      }
      // The case of full recovery:
      // modify the second nibble of final PAD_L 0xBB byte to
      // contain 0x0A as per the scheme
      EM[delta - 1] ^= (byte) 0x01;
      EM[0] = (byte) 0x0b;
      EM[0] |= PADLFIRSTNIBBLE;
    } else {
      //The case of partial recovery: no B bytes required
      // So update the second nibble of final and first PAD_L byte
      // to contain 0x0A as per the scheme
      EM[0] = (byte) 0x0a;
      EM[0] |= PADLFIRSTNIBBLE;
    }
    EM[emLen - 1] = PADR;
    return EM;
  }
\end{lstlisting}

The full class can be found at application/modules/signatures/models//schemes/ISO\_IEC\_9796\_2\_SCHEME\_1.java.



\section{Motivation for Provable Security}
The hash-and-sign paradigm shows potential in countering known attacks by enabling the admission of a function H not efficiently invertible. However this is no replacement for formal proof. This is especially evident considering the Multiplicative property that hash-and-sign methods inherit from textbook RSA. It becomes unfeasible to imagine how $H(m_{1}) \cdot H(m_{2}) \bmod N$ could have the algebraic structure required to make it look like the hash of some other distinct message m. While at the very most a solution to this problem is not evident, this is not a proof that the attack is impossible and thereby immaterial.

For a provably secure signature in the UF-CMA sense, a Hash function \(H\) that avoids multiplicative relations is needed. Moreover a stronger assumption to make would be: it must be hard to find collisions in H. Currently, there's no way to ensure the basic hashed RSA signature scheme is provably secure in standard settings.

The Hash-and-Sign scheme focuses on countering known threats and is based on classical design principles. While identifying essential features of the hash function offers some insight, it's just a starting point. Over-reliance on known threats is risky, as unforeseen flaws can exist due to unidentified features, potentially compromising security.

A better strategy is to understand how a signature scheme's security relates to the underlying primitive's assumed security, like the RSA Assumption. The focus should extend beyond just countering known threats to addressing all potential threats, even unknown ones. Under this very approach the (basic) hashed RSA signature scheme becomes unacceptable. This related philosophy drives the concept of provable security.


\subsection{The difficulty of proving security of RSA PKCS\#1 v1.5 signatures}

Provable security requires linking the security of a signature scheme to the assurance of its underlying primitives. For RSA signatures, this necessitates tying the security of the signature scheme to the hardness of the RSA problem, and some other security condition (collision resistance) on the hash function.

A complication arises with the PKCS\#1 v1.5 signature scheme, stemming from potential message representatives described by the set
\[ S_N = \{ (PAD \| z) \mid \sigma = (PAD \| z)^{1/e} \bmod N \}. \]

Given a hash function like SHA-1 yielding $l$ bits, the magnitude of $S_N$ is bounded by $2^l$. For example with SHA-1, this would mean  $|S_N| \leq 2^{160}$. Despite its seeming breadth, $S_N$ is insignificant within the RSA domain \( Z_N^* \), insinuating a negligible chance for random elements from \( Z_N^* \) to belong to \( S_N \).

This highlights a concern: it becomes feasible to compute e-th roots for values within $S_N$. This differential hardness poses a challenge. If the RSA function can be easily inverted on  $S_N$, then signatures can be forged, and the security of the PKCS signature scheme can be compromised. Hence, relying simply on the RSA assumption does not directly ensure the security of the PKCS signature scheme. 

The vulnerability is not necessarily in the hash function but in how the message, after hashing, is combined with deterministic padding to land in \( S_N \). Because \( S_N \) lacks a known algebraic structure, conventional proof methods do not work. The deterministic nature of \( S_N \) might expose the scheme to potential attacks, making the subset's predictability a risk. To ensure security, the RSA assumption would also need to apply more specifically to  $S_N$. Even then, this is merely a necessary condition, not a guarantee of the signature scheme's security. 

In summary, the lack of known attacks does not guarantee the security of the PKCS scheme or any cryptographic system. For the signature schemes examined in this project, design flaws with RSA usage can be pinpointed. Such flaws deviate from the understanding of the security of RSA based on its accompanying problem and this is cause for concern.


\subsection{Limitations of Provable Security}
As central as the paradigm of Provable Security is, it is important to keep in mind some of its limitations. It does not entail security absolute sense; rather, it depends on specific assumptions. In practice, actual security might differ from what is provable due to real-world vulnerabilities not covered in theoretical models. This means that security claims may not fully encompass real-world scenarios and a cryptographic primitive deemed theoretically unbreakable may still have unrelated vulnerabilities when put into practice. To formulate security definitions that provide meaningful guarantees, it is thus necessary to understand how primitives are used in practice.

For instance, while indistinguishability theory posits that encrypted plaintexts of the same length should be indiscernible, real-world scenarios might allow deductions from ciphertext lengths, potentially compromising confidentiality.  Understanding the security that a proof provides for a specific cryptographic scheme is different from understanding the scheme overall security requirements. Often, real-life applications demand security requirements that exceed what theoretical proofs cover. Moreover, the adversary models used in theoretical proofs may not fully capture the abilities of genuine adversaries. Attacks like padding oracle attacks, where adversaries glean plaintext information, are not always accounted for in security models. Similarly, side-channel attacks exploit external information beyond a model's scope. The relevance of a theoretical model hinges on how closely it simulates the strategies of real attackers.

Implementing cryptographic systems is intricate, with design, integration, and coding all introducing potential pitfalls. While specifications may offer some leeway for those implementing the system, lacking clear and thorough instructions could lead them to make decisions that significantly compromise the security. Even a slight tweak to a proven system can drastically compromise its security. Thus, it's paramount that real-world applications adhere to their theoretical models as closely as possible.


\subsection{Benefits and Real World Implications}
\label{provBenefits}
The ad hoc security approach, relying on attack obscurity, is not only flawed but misleading, as seen in PKCS's encryption scheme within the PKCS\#1 V1.5 standard from 1993 \cite{rfc2313}. Despite rigorous scrutiny, schemes once thought secure were eventually breached. This paved the way for provable security, emphasising concrete security principles and the quality of cryptographic proofs.

By 1998, Bleichenbacher showcased an adaptive chosen-ciphertext attack on the PKCS\#1 v1.5 encryption scheme \cite{bleichenbacher1998chosen}. Here, the decryption determined plaintext's validity based on padding. With incorrect padding, an error message was returned. This was exploited in some SSL/TLS protocol implementations, allowing adversaries to decipher plaintexts from error message information. This attack had profound implications, including a full class subsequent attacks, even up to two decades later (e.g., \cite{coppersmith1996low, coron2000new, 10.1007/978-3-540-45238-6_33, degabriele2012joint, bardou2012efficient, meyer2014revisiting, zhang2014cross, jager2015security, jager2015practical, bock2018return}).

The reach of Bleichenbacher's attack extended to incorrect implementations of the PKCS signature scheme \cite{finney2006bleichenbacher, kuhn2008variants}, enabling digital signature forgery and even more devastatingly, potentially allows for the impersonation of a vulnerable server without even requiring the respective private key. A real-world example targeting Facebook  \cite{bock2018return} in 2018 underscored the risk. Though Meta since rectified their specific TLS implementation, the issue was widespread, affecting almost a third of the top 100 domains in the Alexa Top 1 Million list, and additionally many major domains and products from key vendors and open-source initiatives.

Other versions of the attack exploited code flaws in non-patched OpenSSL versions \cite{CVE-2006-4339}. Some attacks bridged protocols and versions, targeting servers indirectly connected to vulnerable ones, highlighting the challenges of implementing RSA with PKCS\#1 v1.5 padding. These examples underscores the pitfalls of an ad-hoc security approach and the necessity for rigorous, provable security measures to ensure cryptographic schemes can withstand evolving threats over time.


\section{Random Oracle Model}
\label{subSec:ROM}
The complexity of finding proofs for Hash-and-Sign schemes has been discussed but ultimately there are now proofs that apply in the random oracle model (ROM). The ROM \cite{10.1145/168588.168596} is essentially an idealised theoretical model providing access to a hash function H, which is treated as a blackbox (for example, by calling it as a subroutine or by communicating with another computer program). This in turn allows you to prove a protocol to be correct assuming that H maps each input to a truly random output, i.e., it behaves like a truly random oracle. 
The oracle also “remembers” all inputs and if the same input is given, it produces the same output. 

It can be said the hash function is “full-domain” because its values range through the full interval of the modulus i.e., [0, N] enabling messages to be hashed in a way that makes the resulting points more evenly distributed across the full RSA domain. This is an effective mitigation acting as resolution to the basic hash issue of messages being clumped in a tiny and predictable subset. 
Although truly random functions cannot be implemented in reality, the resulting soundness the ROM facilitates in a scheme’s design allows some measure of confidence to derived at the very least. Loosely it provides a guarantee that a scheme is not flawed, based on the intuition that an attacker would be forced to use the hash function in a non generic way. This technique is the starting point, providing the foundational setting for the proof of deterministic signature schemes.


\chapter{Security Proofs}
Due to the breakthrough in 2018 when a security proof \cite{jager2018security}  was provided for RSASSA-PKCS1-v1.5. It is possible to formally prove the security of the full class of deterministic RSA signatures. The precise statement of security is if the RSA problem is hard when H is modelled as a random oracle, then the scheme is secure (with regards to the UF-CMA notion i.e.,  existential unforgability against adaptive chosen message attacks). There are two different proofs for this, both which consider case where the modulus $\widehat{N}$ is a product of three primes. The first is based on the RSA assumption, the second is based on the $\phi$-Hiding assumption.


\section{Encode Algorithm}


Jager, Kakvi and May overcame the difficulty of providing proof for PKCS with a specialised encode algorithm that allows the simulation of signatures in polynomial time.  

\begin{definition} (\textit{Encode} \cite{jager2018security}).
Let
\[
(\widehat{y}, s, z) \xleftarrow{\$} \text{Encode}(N, e, y, \ell, \text{PAD}, R)
\]
be an efficient algorithm that takes as input an \(n\)-bit integer \(N\), an exponent \(e\), \(y \in \mathbb{Z}_N^*\), a hash value length $\ell$, padding pattern PAD and an \(r\)-bit prime \(R \in \mathbb{P}[r]\), and outputs \((\widehat{y}, s, z) \in \mathbb{Z}_{\widehat{N}} \times \mathbb{Z}_N^* \times \{0, 1\}^\ell\) or failure \(\bot\). \

\end{definition}

The algorithm outputs \((\widehat{y}, s, z)\) such that \(\widehat{y}\) has to be correct form for a PKCS\#1 signature mod \(\widehat{N}\) where \(z\) constitutes message hash and \(s\) comprises an \(e\)th root mod \(N\). Using this and the knowledge of \(R\), the \(e\)th root modulo \(\widehat{N}\) can be computed. More precisely it enables the encoding of an arbitrary integer y modulo N as an integer $\hat{y}$ modulo $\hat{N} = NR$, for some prime R, such that $\widehat{y}$ has correct PKCS\#1-V1.5 padding modulo $\hat{N}$.

\begin{enumerate}
\item y denotes an embedded RSA challenge that given a forgery can be solved i.e., obtaining \(\widehat{y} = PAD\|\|z \). 
\item In the case that \(y\) is replaced by 1 the algorithm instead stimulates a signature. 
\end{enumerate}
With these two uses of the Encode algorithm, UF-CMA security of PKCS\#1 was proved in the ROM. 

\begin{figure}[H]
\centering
\[
\boxed{
\begin{aligned}
&\underline{\textbf{Encode} (N, e, y, \ell, \text{PAD}, R)} \\
&n = \left\lceil \log_2 N \right\rceil, \; r = \left\lceil \log_2 R \right\rceil \\
&z := 2^\ell, \; k := 0 \\
&\textbf{while } (z \geq 2^\ell) \textbf{ and } (k < n \cdot 2^{-\ell}): \\
&\quad k := k + 1 \\
&\quad s \xleftarrow{\$} \mathbb{Z}_N \\
&\quad z := y^s e - 2^\ell \cdot \text{PAD} \mod N \\
&\quad \widehat{y} := 2^\ell \cdot \text{PAD} + z \\
&\textbf{if } z < 2^\ell \textbf{ then} \\
&\quad \textbf{return } (\widehat{y}, s, z) \\
&\textbf{else} \\
&\quad \textbf{return } \perp.
\end{aligned}
}
\]
\caption{Encode algorithm \cite{jager2018security}}
\label{fig:encode}
\end{figure}

The algorithm outputs \((\widehat{y}, s, z)\) except with negligible failure probability (in n). It can be said that encode efficiently $(n - \mathcal{O}(log \text{ } n))$-simulates the PKCS\#1 v1.5 encoding modulo $\hat{N} = NR$ which is true for a large hash value $\ell \approx |n|$.

\begin{comment}
We remark that the above proof sketch is slightly different from our actual proof, although the main proof idea remains identical. In fact, we use the more sophisticated approach of Coron [Cor00] to embed y in multiple hash values. This achieves a better tightness bound, where the loss is only linear in the number of signature queries qs and not in the number of hash queries qh.
\end{comment}


\section{Background}
See section \ref{subSec:RSA-ASS} for the intuition behind trapdoor permutations in the context of its application to RSA.
\begin{definition}
\label{def:TDP}
A \textit{family of trapdoor permutations} \cite{10.1145/147508.147537}. \( TDP = (\text{Gen}, \text{Eval}, \text{Invert}) \) comprises the following three polynomial-time algorithms:
\begin{enumerate}
    \item \textbf{Gen:} A probabilistic algorithm that, given input \( 1^{\lambda} \), produces a public description \text{pub} (inclusive of an efficiently sampleable domain \( \text{Dom}_{\text{pub}} \)) and a trapdoor \( td \).
    
    \item \textbf{Eval:} A deterministic algorithm that, given \text{pub} and \( x \in \text{Dom}_{\text{pub}} \), yields \( y \in \text{Dom}_{\text{pub}} \). This relationship is expressed as \( f(x) = \text{Eval}(\text{pub}, x) \).
    
    \item \textbf{Invert:} A deterministic algorithm that, given \( td \) and \( y \in \text{Dom}_{\text{pub}} \), produces \( x \in \text{Dom}_{\text{pub}} \). This relationship is described by \( f^{-1}(y) = \text{Invert}(\text{pub}, y) \).
\end{enumerate}

It is required for all \( \lambda \in \mathbb{N} \) and any (\text{pub}, td) produced by \(\text{Gen}(1^{\lambda})\):
\[ f(\cdot) = \text{Eval}(\text{pub}, \cdot) \text{ must define a permutation over } \text{Dom}_{\text{pub}} \]
and additionally for all \( x \in \text{Dom}_{\text{pub}} \):
\[ \text{Invert}(td, \text{Eval}(\text{pub}, x)) \text{ should equal } x \]

It's important to note that \( f_{\text{pub}}(\cdot) = \text{Eval}(\text{pub}, \cdot) \) needs to be a permutation for a correctly generated \text{pub}. 
\end{definition}

A trapdoor permutation is certified if one can publicly verify that it actually defines a permutation. 

\begin{definition} \textit{Certified Trapdoor permutation} \cite{10.1007/3-540-48071-4_31, bellare1996certifying, 10.1007/978-3-642-34961-4_25}.
\label{def:CTDP}
A family of trapdoor permutations \( \text{TDP} \) is called \textit{certified} if there exists a deterministic polynomial-time algorithm \text{Certify} that, on input of \( 1^{\lambda} \) and an arbitrary (polynomially bounded) bit-string \( \text{pub} \) (potentially not generated by \text{Gen}), returns 1 iff \( f(\cdot) = \text{Eval}(\text{pub}, \cdot) \) defines a permutation over \( \text{Dom}_{\text{pub}} \).
\end{definition}

Lossy Trapdoor Permutations are a realisation of the lossiness security notion for trapdoor permutations. Essentially, these permutations function in two distinct modes. The first allows for complete input recovery using an (injective) trapdoor function, while the second ((lossy) trapdoor function) causes substantial input data loss. Notably, distinguishing between these two behaviours is hard for any efficient adversary.

\begin{definition}
\textit{Lossy trapdoor permutation} \cite{10.1145/1374376.1374406, kakvi2018optimal}. Let \( l \geq 2 \). A trapdoor permutation \( \text{TDP} \) is a \( (l, t, \varepsilon) \) \textit{lossy trapdoor permutation} if the following two conditions hold:

\begin{enumerate}
    \item There exists a probabilistic polynomial-time algorithm \text{LossyGen}, which on input \( 1^\lambda \) outputs \( \text{pub}' \) such that the range of \( f_{\text{pub}'}(\cdot) := \text{Eval}(\text{pub}', \cdot) \) under \( \text{Dom}_{\text{pub}'} \) is at least a factor of \( l \) smaller than the domain \( \text{Dom}_{\text{pub}'} \): 
    \[ \frac{\text{Dom}_{\text{pub}'}}{f_{\text{pub}'}(\text{Dom}_{\text{pub}'})} \geq l \]
    
    \item All distinguishers \( \mathcal{D} \) running in time at most \( t \) have an advantage \( \text{Adv}^L_{\text{TDP}}(\mathcal{D}) \) of at most \( \varepsilon \), where
    \[ \textbf{Adv}^L_{\text{TDP}}(\mathcal{D}) = \Pr[\textbf{L}_{1}^{\mathcal{D}}  \Rightarrow 1] - \Pr[\textbf{L}_{0}^{\mathcal{D}} \Rightarrow 1] \]
\end{enumerate}
\end{definition}

\begin{figure}[H]
\[
\boxed{
\begin{aligned}
&\underline{\textbf{\text{procedure Initialise Game } \(L_0\)}} \\
&(pub, td) \xleftarrow{\$} \text{Gen}(1^\lambda) \\
&\text{return } pub \\
\\
&\underline{\textbf{\text{procedure Initialise Game } \(L_1\)}} \\
&(pub', L) \xleftarrow{\$} \text{LossyGen}(1^\lambda) \\
&\text{return } pub' \\
\end{aligned}
}
\]
\caption{The Lossy Trapdoor Permutation Games.}
\label{fig:lossy_trapdoor_permutation_games}
\end{figure}

\begin{definition}
\label{def:reg-los}
\textit{Regular lossiness} \cite{10.1145/1374376.1374406, kakvi2018optimal}. A TDP is regular \( (l, t, \varepsilon) \) lossy if the TDP is \( (l, t, \varepsilon) \) lossy and all functions \( f_{\text{pub}'}(\cdot) = \text{Eval}(\text{pub}', \cdot) \) generated by \text{LossyGen} are \( l \)-to-\( 1 \) on \( \text{Dom}_{\text{pub}'} \).
\end{definition}

\begin{definition} 
\textit{The RSA trapdoor permutation} \cite{kakvi2018optimal}. \texttt{RSA} = (\texttt{RSAGen}, \texttt{RSAEval}, \texttt{RSAInv}): 
\begin{enumerate}
\item \texttt{RSAGen}($1^\lambda$) outputs \texttt{pub} = (N,e) and \texttt{td} = d, where $N = pq$ is the product of two $\lambda/2$-bit primes, \texttt{gcd}(e,$\phi(N)$) = 1, and $d = e^{-1} \mod \phi(N)$. 
\begin{itemize}
\item The domain is $\texttt{Dom}_{\texttt{pub}} = \mathbb{Z}_N^*$. 
\end{itemize}
\item \texttt{RSAEval}(\texttt{pub}, x) returns $f_{\texttt{pub}}(x) = x^e \mod N$, 
\item \texttt{RSAInv}(\texttt{td}, y) returns $f_{\texttt{pub}}^{-1}(y) = y^d \mod N$. 
\end{enumerate}

\end{definition} 

In the context of \texttt{RSA}, lossy trapdoor permutations can be viewed as the converse of certified trapdoor permutations. For example they entail an impossibility to differentiate an honestly generated (N, e) from (N, $\hat{e}_{loss}$) for which $RSA_{N, \hat{e}_{loss}}$ is many-to-1, thereby disqualifying themselves as permutations.


\subsection{2v3PA}
A final step computation assumption relevant to security statements in both proofs is the 2 vs 3 primes assumption \texttt{2v3PA}[$\lambda$]. It postulates that it's hard to discern if a specific modulus is derived from two or three prime factors. Although the assumption has never been formally studied it is widely accepted. Its role is to bridge the proofs to the setting of the typical two prime factor modulus.

\begin{definition}
 \label{def:2v3}
\textit{The 2 vs. 3 Primes Assumption} \cite{jager2018security} . The \texttt{2v3PA}[$\lambda$] states that it is hard to distinguish between $N_2$ and $N_3$, where $N_2$, $N_3$ are $\lambda$-bit numbers, where 
\begin{enumerate}
\item $N_2 = p_1 p_2$ is the product of 2 distinct random prime numbers $p_1, p_2 \in \mathbb{P}$; 
\item $N_3 = q_1 q_2 q_3$ is the product of 3 distinct random prime numbers $q_1, q_2, q_3 \in \mathbb{P}$
\end{enumerate}
\texttt{2v3PA}[$\lambda$] is said to be $(t, \epsilon)$-hard, if for all distinguishers $\mathcal{D}$ running in time at most $t$, we have:
\[
\textbf{Adv}^{\texttt{2v3PA}[\lambda]}_\mathcal{D} = \Pr[1 \leftarrow \mathcal{D}(N_2)] - \Pr[1 \leftarrow \mathcal{D}(N_3)] \leqslant \epsilon
\]
\end{definition}

\section{Proof Theorems}
Both proofs consider signatures of the form \[ \sigma = (\gamma \cdot H(m) + f(m))^{1/2} \] i.e., a scheme with (potentially) message-dependent padding. This stems from theorem statements used initially by Coron's proof \cite{coron2002security} for the Rabin-Williams variant (\( e = 2 \)) of RSASSA-PKCS1-v1.5. Coron's theorem statements were general enough to indeed be extended to the ANSI X9.31 rDSA and ISO/IEC 9796-2 Scheme 1 signatures. Therefore while not explicitly stated, it can be said that the considered proofs, essentially descendants of Coron's proof, also apply to the latter two schemes, with relevant adjustments.
\begin{table}[H]

\begin{tabular}{|m{4.38cm}|m{5cm}|m{1.5cm}|m{4cm}|}
\hline
\textbf{Signature Scheme} & \textbf{Message Representative} & \textbf{$\gamma$ value} & \textbf{\( f(m) \) Value} \\ \hline
PKCS\#1 v1.5              & \( y = PAD  \| z \)      & \(1\) & \( \texttt{PAD} \times 2^{\ell} \) \\ \hline
ANSI X9.31 rDSA           & \( y = PAD  \| z \| ID_{H} \)          & \( 2^{16} \)     & \( \texttt{PAD} \times 2^{\ell + 16} + \texttt{ID}_{\texttt{H}} \)     \\ \hline
ISO/IEC 9796-2 Scheme 1   & \( y = PAD_{L} \| m_{1} \| z \| PAD_{R} \)            & \( 2^{8} \)       & \( (\texttt{PAD}_{\texttt{L}} \| m_{1} \times 2^{\ell + 8}) + \texttt{PAD}_{\texttt{R}} \)       \\ \hline
\end{tabular}
\caption{Signature Schemes considered in the context of Coron's proof \cite{coron2002security}}
\label{table:coron}
\end{table}




\subsection{Security Proof under the RSA Assumption}
The computation assumption relevant to security statements in the first proof is the RSA Assumption \text{k-}\texttt{RSA}[$\lambda$] (See definition \ref{def:RSA-ASS}).

\begin{theorem}
(Jager-Kakvi-May \cite{jager2018security}). Assume that 2-RSA[\(\lambda\)] is (\(t', \varepsilon'\))-hard. Then for any (\(q_h, q_s\)), RSASSA-PKCS1-v1.5 is (\(t, \varepsilon, q_h, q_s\))-UF-CMA secure in the Random Oracle Model, where
\[
\varepsilon' = \frac{\varepsilon}{q_s} \cdot \left(1 - \frac{1}{q_s + 1}\right) \approx \frac{\varepsilon}{q_s} \cdot \exp(-1)
\]

\[
t' = t + \mathcal{O}(q_h \cdot \lambda^4).
\]
The proof requires the following adaptations to bounds for applicability to the remaining schemes.
\begin{table}[H]
\centering
\caption{Adaptation of the 2-RSA[\(\lambda\)] proof bounds to ANSI and ISO Schemes}
\begin{tabular}{|m{4.38cm}|m{1.5cm}|m{4.5cm}|}
\hline
\textbf{Signature Scheme} & \textbf{$\gamma$ value} & \textbf{Modified $t'$ bound} \\ \hline
ANSI X9.31 rDSA               & \( 2^{16} \)     & \( t' = t + 2^{15} \cdot \mathcal{O}(q_h \cdot \lambda^4) \)     \\ \hline
ISO/IEC 9796-2 Scheme 1        & \( 2^{8} \)       & \( t' = t + 2^7 \cdot \mathcal{O}(q_h \cdot \lambda^4) \)       \\ \hline
\end{tabular}
\label{table:rsaAdap}
\end{table}
\end{theorem}

The reductions used in the first proof bounds the probability \( \varepsilon \) of breaking \texttt{RSASSA-PKCS-V1.5} in time \( t \) by \( \varepsilon' \cdot q_s \), where \( \varepsilon' \) is the probability of inverting RSA in time \( t' \approx t \) and \( q_s \) is the number of signature queries by the forger. Equivalently, the security reduction is loose with a loss in the order of \( q_s \). This is not ideal since it has a negative impact on the practical parameter choices for instantiations. 

A realisation of this is the necessity for a significantly larger modulus relative to what can be achieved with a tight reduction to achieve a specific level of security, or more generally, obtaining a meaningful security proof. The loss is optimal for RSA with “large” exponents (\cite{10.1007/3-540-46035-7_18}, \cite{kakvi2018optimal}). More precisely this refers to an exponent \( e > N \) that is additionally prime \cite{10.1007/3-540-48910-X_28, 10.1007/978-3-540-24676-3_5}. This is due to the fact that such an exponent defines RSA as a Certified Trapdoor Permutation. This is because if \( e \) is a prime, then it can never divide \( \phi(N) < N \) and hence \( \gcd(e, \phi(N)) = 1 \). 

In this case the 2-RSA[\(\lambda\)] proof implies SUF-CMA security as well as resilience against subversion attacks \cite{ateniese2015subversion}. However this discovery serves primarily as a theoretical/initial insight and design validation since choosing \( e > N \) is usually avoided in practice due to the costs for modular exponentiation. 

\begin{comment}
First, [5,20] observed that if \( e > N \) and \( e \) is prime, then the RSA function \( RSA_{N,e} \) is a certified permutation. This is because if \( e \) is a prime, then it can never divide \( \phi(N) < N \) and hence \( \gcd(e, \phi(N)) = 1 \). However, choosing \( e > N \) is usually avoided in practice due to the costs for modular exponentiation. That is, every security reduction from inverting the TDP (i.e., RSA in the case of...) to breaking PKCS signatures will inevitably lose a \( q_s \) factor.
\end{comment}

\subsection{Security Proof under the Phi-Hiding Assumption}
The computation assumption relevant to security statements in the second proof is the $\varphi$-Hiding Assumption k-$\phi$HA[$\lambda$]. 
It posits that for a given modulus \( N \) and a sufficiently small exponent \( e \) (\( e < N^{\frac{1}{4}} \)), determining if \(\frac{\phi(N)}{e}\) is hard. 

By definition \ref{def:reg-los} when gcd(\( \hat{e}_{\text{loss}} \),\( \phi(N) \)) = \( \hat{e}_{\text{loss}} \) the $RSA_{N, \hat{e}_{loss}}$ function is \( \hat{e}_{loss}{\text{-to-}}1 \) i.e., \( \hat{e}_{\text{loss}}- \)regular lossy.

\begin{definition}
\textit{The $\varphi$-Hiding Assumption} \cite{10.1007/3-540-48910-X_28}. The k-$\phi$HA[$\lambda$], states that it is hard to distinguish between $(N,e_{inj})$ and $(N,\hat{e}_{loss})$, where 
\begin{enumerate}
\item $N$ is a $\lambda$-bit number such that $N = \displaystyle\prod_{i=1}^{k} p_i$ i.e., the product of k distinct random prime numbers $p_i \in_{R} \mathbb{P}[\lambda_i]$, for $i \in \llbracket1, \ldots, k \rrbracket$, for k constant.  
\item $e_{inj}, \hat{e}_{loss} > 3 \in \mathbb{P}$;
\item $e_{inj}, \hat{e}_{loss} \leq N^{1/4}$, with $\gcd(e_{inj}, \varphi(N)) = 1$ and $\gcd(\hat{e}_{loss}, \varphi(N)) = \hat{e}_{loss}$, where $\varphi$ is the Euler Totient function. 
\end{enumerate}
k-$\phi$\texttt{HA}[$\lambda$] is said to be $(t, \epsilon)$-hard, if for all distinguishers $\mathcal{D}$ running in time at most $t$, we have:
\[
\textbf{Adv}^{\text{k-}\phi \texttt{HA}[\lambda]}_\mathcal{D} = \Pr[1 \leftarrow D(N, e_{inj})] - \Pr[1 \leftarrow D(N,\hat{e}_{loss})] \leqslant \epsilon
\]
\end{definition}

\begin{theorem}
(Jager-Kakvi-May \cite{jager2018security}). Assume \(2\)-\(\Phi\)HA[\(\lambda\)] is \((t', \varepsilon')\)-hard and gives an \(\eta\)-regular lossy trapdoor function. Then, for any \((q_h, q_s)\), \text{RSASSA-PKCS1-v1\_5} is \((t, \varepsilon, q_h, q_s)\)-UF-CMA secure in the Random Oracle Model, where
\[
\varepsilon = \left(\frac{2\eta - 1}{\eta - 1}\right) \cdot \varepsilon'
\]
\[
t = t' + \mathcal{O}(q_h \cdot \lambda^4)
\]
The proof requires the following adaptations to bounds for applicability to the remaining schemes.
\begin{table}[H]
\centering
\caption{Adaptation of the 2-RSA[\(\lambda\)] proof bounds to ANSI and ISO Schemes}
\begin{tabular}{|m{4.38cm}|m{1.5cm}|m{4.5cm}|}
\hline
\textbf{Signature Scheme} & \textbf{$\gamma$ value} & \textbf{Modified $t$ bound} \\ \hline
ANSI X9.31 rDSA               & \( 2^{16} \)     & \( t = t' + 2^{15} \cdot \mathcal{O}(q_h \cdot \lambda^4) \)     \\ \hline
ISO/IEC 9796-2 Scheme 1        & \( 2^{8} \)       & \( t = t' + 2^7 \cdot \mathcal{O}(q_h \cdot \lambda^4) \)       \\ \hline
\end{tabular}
\label{table:phiAdap}
\end{table}
\end{theorem}


The second proof bypasses the optimality result of the 2-RSA[\(\lambda\)] proof (i.e., tight security proof that is independent of $q_s$), by using "small" keys that do not define a certified trapdoor permutation. More precisely "small" keys, entail an exponent \( e \leqslant N^{1/4} \) (i.e., e is at most $\frac{1}{4}$ of the bit-length of N). This allows for a security proof tight to the $\varphi$-Hiding assumption. Additionally, there is currently no known proof technique which achieves tighter security for small exponents.

The condition that $e_{inj}, \hat{e}_{loss} \le N^{\frac{1}{4}}$ arises because with only the modulus N known, it is not possible to determine the number of prime factors it has or their relative sizes. It is understood that N is composite, implying it has a minimum of two prime factors. The bound then follows as a direct consequence of known attacks described by Coppersmith \cite{coppersmith1997small}.

Such exponents define RSA as a Lossy Trapdoor Permutation. This is because when gcd(\( \hat{e}_{\text{loss}} \),\( \phi(N) \)) = \( \hat{e}_{\text{loss}} \) the $RSA_{N, \hat{e}_{loss}}$ function is \( \hat{e}_{loss}{\text{-to-}}1 \) i.e., \( \hat{e}_{\text{loss}}- \) regular lossy (see Definition \ref{def:reg-los}).  In this case the \(2\)-\(\Phi\)HA[\(\lambda\)] proof implies only the weaker UF-CMA security and no provable security against subversion attacks. The proof technique has direct relevance to real-world instantiations often in which small exponents e.g., $e = 2^{16} + 1$ are used.

\begin{comment}
Coron’s proof has a security loss that depends on the number of signing queries qs, as opposed to the number of hash queries \( q_h \), which is generally much larger. Additionally, Coron showed that this proof was actually optimal by means of a meta-reduction [12], removing any hope of a tight proof.
However, Kakvi and Kiltz [38] noticed that the meta-reduction crucially requires that the RSA public key \( (N, e) \) be a CTDP [5, 6]. We say a TDP is certified if there is a polynomial time algorithm that verifies that the public evaluation parameters of the trapdoor permutation are well formed i.e., that they do indeed define a permutation. This is not the case for RSA if the exponent \( e \) is a small prime. In particular, if \( e < N^{1/4} \) then this defines an LTDP [50] under the \(\phi\)-Hiding Assumption [9].

Such optimizations that speed up decryption are of great importance because it is often the case that a server needs to carry out many decryption operations simultaneously.

Small Exponents
As already noticed, people may be interested in using small exponents, either a small encryption exponent to speed up the encryption process, or a small decryption exponent to speed up the decryption process.
A short encryption exponent may be dangerous, as already remarked above, but even in a single-user envi- ronment if the message to be encrypted is short too: Let us assume that one uses a -bit modulus n for a quite secure application (such as electronic transactions), with public exponent e = . hen, one uses this cryptosystem to encrypt a short message m (a transaction, a credit card number, etc.) over less than  characters (i.e., less than 
bits). he ciphertext is c = m mod n,buteveninZ since m is over , bits only which is less than n:themodu- lar reduction does not apply – it may be faster! But on the other hand, to recover m from c,onejusthastocomputea third root in Z, which does not require any factorization.
This in particular includes the important low-exponent cases of e = 3 and e = 216 + 1 since they allow efficient verification of RSA-FDH signatures.3
In practice, e is chosen to be small and is generally fixed to some specific numbers, such as e = 3 or e = 216 + 1, which allows for fast exponentiation

and therefore, the impossibility result does not apply to all instances of RSA-FDH. In particular, it is
common practice to use either e = 3ore = 216 + 1, which are less then N 1/4. 

For efficiency reasons, the public RSA exponent e is typically not chosen to be too large in practice. (For example, researchers at UC San Diego [63] found that 99.5 of the certificates in the campus’s TLS corpus had e = 216 + 1.) Therefore, we investigate the possibility of using an additional assumption to “amplify” the lossiness of RSA for small e.

To speed signature verification, a widely-employed strategy is to select a small encryption exponent e, such as e 3 or e 216 1. With such a choice, RSA signature verification is much faster than RSA signature generation. Note that RSA signature generation cannot be sped up by selecting a small decryption exponent d without incurring a significant security risk [29, 4].
such as e = 216 + 1, as commonly used in real-world instantiations 

e<N1/4 (e.g., using the common choices e =3 or e =216 + 1)

Speeding up Encryption and Decryption. The modular exponentiation algorithm is especially efficient, if the exponent has many zeros in its binary encoding. For each zero we have one less multiplication. We can take advantage of this fact by choosing an encryption exponent e with many zeros in its binary encoding. The primes 3, 17 or 216 + 1 are good examples, with only two ones in their binary encoding.

\end{comment}



\section{Implications for practical instantiation}

\subsection{Both Proofs}
Considering PKCS\#1 v1.5 signatures instantiated with an $\lambda$-bit modulus:
\begin{implication} \textit{Hash output $\displaystyle|H()| \ge \displaystyle\frac{\lambda}{2}$}.
\label{IMP:HASH}
Both proofs work only with a hash function that has an uncommonly large output. More precisely they work under the assumption that breaking the RSA (or $\varphi$-Hiding) assumption is hard with respect to an $\frac{\lambda}{2}$-bit modulus $N$ with $|N| = |H()|$. Concretely, for signatures instantiated with an 1024-bit RSA modulus $\displaystyle\hat{N}$, this would mean security in the context of a 512-bit RSA assumption, with 512-bit padding and a 512-bit hash function.
\end{implication}

The PKCS\#1 v2.2 standard, includes in Appendix B of RFC 8017 \cite{rfc8017}, a method for transforming a cryptographic hash function to generate outputs of any desired size. This method, known as Mask Generation Function 1 (MGF1) involves the recurrent application of a conventional hash function to the input alongside incrementally changing counter values to achieve the required output length.

Alternatively, the NIST FIPS 202 standard \cite{1421} specifies extendable-output functions (XOFs), called SHAKE-128 and SHAKE-256 under the SHA-3 umbrella, that have arbitrary length output. The method forming the basis for this is known as the Keccak construction \cite{bertoni2011keccak}. It involves two phases: absorption, where input is padded, divided, and XORed with a state, and squeezing, where the state is repeatedly permuted to generate output of desired length.

\begin{comment}
The first of these is the absorbing phase, where a variable-length input is padded and divided into blocks of equal size. Each block is then XORed with a specific memory area called the state, followed by the application of an internal permutation to the state. This process is repeated for each input block, continuously altering the state. The construction then shifts to the squeezing phase where the permutation is applied repeatedly to the state, each time extracting a portion of the state to extend the output. This continues until the output reaches the desired length.
\end{comment}
\begin{implication} \textit{3-Prime Factor Modulus N}.
\label{IMP:3prime}
The Encode algorithm requires the modulus to double in bit length when compared to the norm with a newly introduced third prime factor necessitating the increase in bits. Thus if a \(\lambda\)-bit modulus is used in the key for the security reductions of 2-$\phi$HA[$\lambda$] or 2-RSA[\(\lambda\)], this includes an additional \(\frac{\lambda}{2}\)-bits made up by a third prime factor. In turn the security proofs apply for keys where N = $p_1 p_2 p_3$, i.e., is the product of three primes. Under the assumption that 3-prime moduli are indistinguishable from 2-prime moduli, results can be brought back to the case N = $p q$.
\end{implication}

\subsection{Security Proof under the RSA Assumption}
\begin{implication} \textit{arbitrary-e}.
\label{IMP:RSA}
The proof allows for the selection of an arbitrary-e. Although the general security of a practical instantiation should be considered with respect to the aforementioned security loss (the number of signature queries) which is as discussed is not optimal in this setting. 
\begin{comment}
Furthermore this does not match the setting of practical instantiations where choices of e are commonly fixed and small values for efficiency purposes (see Implication \ref{IMP:PHI}).
\end{comment}
\end{implication}

\subsection{Security Proof under the Phi-Hiding Assumption}
\begin{implication} \textit{prime $e \leqslant \displaystyle2^{\frac{\lambda}{4}}$}.
\label{IMP:PHI}
The proof allows for the selection of a small e. This is ideal, since a widely-employed strategy, in order to enable efficient verification of signatures is to select small and fixed specific numbers such as $e = 3 \text{ or } e = 2^{16} + 1$ (both of which $\le N^{\frac{1}{4}}$). With such a choice, RSA signature verification is much faster than RSA signature generation which is useful because practically signature verification is often the predominant operation being performed.
\end{implication}

\subsection{Summary of Implications}



\begin{table}[H]
\centering
\caption{Parameter sizes and security of deterministic RSA hash-and-sign signatures (instantiated with a $\lambda$-bit modulus) \cite{10.1007/978-3-030-64357-7_5}}
\resizebox{\textwidth}{!}{% Resize table to fit within page margins
{\Large % This sets the font size for the table
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Scheme & Proof methodology & Assumption & No. prime factors & Exponent \( e \) & \( |H(\cdot)| \) & Security loss \\
\hline
\multirow{5}{*}{\begin{minipage}[c]{0.4\textwidth}\textbullet{} RSASSA-PKCS1-v1\_5 \\
\textbullet{} ANSI X9.31 rDSA \\
\textbullet{} ISO/IEC 9796-2 Scheme 1

\end{minipage}} & Jager-Kakvi-May & \texttt{\(2\)-RSA}[\( \lambda /2 \)] & 3 & Arbitrary & \( \geqslant \lambda /2 \) & \( q_s \) \\
\cline{3-7}
&                 & \texttt{\(+ 2\)\(v\)3PA}[\( \lambda \)] & 2 & Arbitrary & \( \geqslant \lambda /2 \) & \( q_s \) \\
\cline{2-7}
& Jager-Kakvi-May & \texttt{\(2\)-\(\phi\)HA}[\( \lambda /2 \)] & 3 & Prime \( \leqslant 2^{\lambda /4} \) & \( \geqslant \lambda /2 \) & \( \mathcal{O}(1) \) \\
\cline{3-7}
&                 & \texttt{\(+ 2\)\(v\)3PA}[\( \lambda \)] & 2 & Prime \( \leqslant 2^{\lambda /4} \) & \( \geqslant \lambda /2 \) & \( \mathcal{O}(1) \) \\
\hline
\end{tabular}
}
}
\end{table}


\chapter{Practical Assumptions for Project Implementation}

\section{Notion of a Provably secure Key Configuration}


\begin{comment}
Taking note of parameter sizes as defined by the considered proofs, there are two potential impacts in relation to the choice of the public exponent e. The second one as per implication \ref{IMP:PHI} of the 2-\(\phi\)HA[\(\lambda\)] proof introduces a constrained selection of \( e \) and helps to facilitate a practical compromise. This recommendation contrasts with the allowance for the selection of any value of e permitted under the 2-RSA[\(\lambda\)] proof.

While the smaller \( e \) value means the e 2-\(\phi\)HA[\(\lambda\)] proof implies only the weaker UF-CMA-security, as opposed to the stronger SUF-CMA security obtained from the 2-RSA[\(\lambda\)] proof, not only is the proof a tight one independent of signature queries, it enables for more efficient verifications of signatures in terms of computational time.
\end{comment}
\begin{comment}
not only is the proof a tight one independent of signature queries, it is in direct alignment with real world applications where it is common to select small values for e to yield efficient verification of signatures.
\end{comment}

There are two potential impacts in relation to the choice of the public exponent e. The second one as per implication \ref{IMP:PHI} of the 2-\(\phi\)HA[\(\lambda\)] proof and its associated compromise (tight proof independent of the number signature queries, but implies only the weaker UF-CMA-security, as opposed to the stronger SUF-CMA security obtained from the 2-RSA[\(\lambda\)] proof) was the one adopted in this project. Consequently, the following key configuration types are defined:

\begin{enumerate}
    \item[] \begin{basic}  \label{PROVKEY}
\textit{Provably secure key configuration}. 2 ($\lambda$/2, $\lambda$/2) or 3 ($\lambda$/4, $\lambda$/4, $\lambda$/2) prime factor modulus encompassed as the key size $\lambda$ for a key generated with the selection of a small e value.
\end{basic}
    \item[] \begin{basic}
\textit{Standard key configuration}. 2 ($\lambda$/2, $\lambda$/2) or 3 ($\lambda$/4, $\lambda$/4, $\lambda$/2) prime factor modulus encompassed as the key size $\lambda$ for a key generated with the selection of an arbitrary e value.
\end{basic}

\end{enumerate}

\begin{comment}
It is important to note that the construction of keys using both 2 and 3 prime factor settings applies uniformly to both standard and provably secure key configurations. This uniformity is a necessary response to implication \ref{IMP:3prime} from both proofs of using a 3 prime factor modulus. It set the stage for further comparative analysis within each parameter type about the effects of utilising an additional prime factor in key construction. This aspect is crucial since the foundational argument of the proofs is that a 3 prime factor configuration (the default for both proofs) can be bridged to the setting of the typical two prime factor modulus using the 2v3PA[$\lambda$].
\end{comment}



\section{Notion of a Provably secure instantiation of a signature scheme}
\label{sec:ProvablySecureInstantiation}


As detailed in Section \ref{hashOutput}, employing a hash output of at least \(\frac{\lambda}{2}\) is essential for provable security in signature schemes. However, this enhanced hash output is not the sole requirement for a scheme's provable security. It must be complemented with a provably secure key configuration as defined in Definition \ref{PROVKEY}. A signature scheme is thus considered provably secure when it combines both a large hash output and a provablt secure key configuration. This leads to the definition of two primary parameter types referenced throughour this report:


\begin{comment}
In Section \ref{hashOutput}, details are provided regarding the enabling of a Provably Secure hash output (e.g., ( \(\geq \frac{\lambda}{2} \)) in the considered signature schemes. It is crucial to understand that while an enhanced hash output within these schemes is necessary for provable security, it alone does not suffice to classify the instantiation of a signature scheme as provably secure. This concept is closely tied to the notion of provably secure key configurations. If a signature scheme, which is configured to use a provably secure hash output before executing any signature operations, is initially constructed using a key with a provably secure key configuration (as defined in basic definition \ref{PROVKEY}), then the scheme can be classified as instantiated with provably secure parameters. Together, these two factors constitute what is known as a provably secure parameter type. With this understanding, we can now define the two primary parameter types referenced throughout this report:
\end{comment}
\begin{enumerate}
    \item[] \begin{basic} 
\textit{Instantiation of a scheme with standard parameters}. A scheme initialised using a $\lambda$-bit key generated using 2 or 3 prime factors and arbitrary e value configured to use a fixed size hash function e.g., a commonly used option is SHA-256.
\end{basic}
    \item[] \begin{basic}
\textit{Instantiation of a scheme with provably secure parameters}. A scheme initialised using a $\lambda$-bit key generated using 2 or 3 prime factors and small e value configured to use a hash function with an \textit{output $\displaystyle|H()| \geq \displaystyle\frac{\lambda}{2}$}.
\end{basic}

\end{enumerate}
As with key configuration, the utilisation of both 2 and 3 prime factor setups extends to the general parameter types used to classify the instantiation of schemes. It is important to note that while a provably secure hash output could be any anything \( \geq \frac{\lambda}{2} \) for the purposes of analysis in this project it is fixed to the preset proportion \( \frac{\lambda}{2} \).




\chapter{Main Deliverable}



\textcolor{red}{*Appendix A presents my approach to software development and a discussion of technology choices relevant to implementation detail.}



\section{Requirements Specification}
The overall goal of the system is to provide a benchmarking tool designed to quantify the computational time impact of implementing larger, provably secure parameters in the full suite of deterministic RSA signature schemes (albeit with a main focus on the PKCS standard), as opposed to the standard parameters commonly used. This program will be equipped with functionalities that allow users to tailor where relevant the parameters to their specific requirements before starting any benchmarking activities on large scale batches of data. Benchmarking activities will span the full digital signature workflow from key generation and signature creation, to signature verification.
\subsection{Description of Actors}

\begin{table}[H]
    \centering
    \caption{Description of Actors}
    \label{tab:actors_description}
    \begin{tabular}{|l|p{10cm}|}
    \hline
    \textbf{Actor / Role Name} & \textbf{Role Description and Objective} \\
    \hline
        User/Signer & The individual in this role seeks to benchmark the signing process. For Key Generation, they select the number of keys,  configure key sizes (e.g., 256, 256, 512) with an optional selection of small e for each, and start key generation benchmarking to run for a number of trials  For signature creation, they supply a message batch, key batch, hash function, and launch benchmarking for their desired signature scheme and. The aim is to assess the efficiency of different signing configurations, including provably secure parameters. \\
    \hline
       Verifier & Responsible for benchmarking the verification process. The verifier provides a batch of signed messages and public keys, chooses a hash function, and starts verification benchmarking. The objective is to understand the verification efficiency of selected signature schemes with different parameter types, including standard and provably secure parameters.  \\
    \hline
    \end{tabular}
\end{table}




\subsection{Use Cases (Benchmarking Mode)}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{main_pictures/MAIN_USE-CASE.png}
    \caption{UML Use Case Diagram}
    \label{fig:uc}
\end{figure}

Please see Appendix Appendix B.1 for a full breakdown of relevant requirements (capturing essential behaviour), and a flow-of-events type description for figure \ref{fig:uc} representing completed UML use cases, derived from preceding user stories.



\section{A Horizontal Slice: Key Generation}
\subsection{Design}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.48]{main_pictures/KeyGeneration.png}
    \caption{Key Generation Class Design}
    \label{fig:kg}
\end{figure}

Figure \ref{fig:kg} provides a conceptualised design view of the foundational GenRSA definition discussed in section  \ref{subSec:keygen} with the process broken down into smaller components corresponding to roles/entities supportive to the purpose of generating keys. 


\begin{table}[H]
    \centering
    \caption{GenRSA Design}
    \label{tab:actors_description}
    \begin{tabular}{|l|p{10cm}|}
    \hline
    \textbf{Class} & \textbf{Description} \\
    \hline
    KeyGenRSA & Central class responsible for generating the key pair by providing a method to fulfil the generation process. \\
    \hline
    Key & Abstract class that serves as a blueprint for the public and private keys, encapsulating the use of the modulus and an accompanying exponent with further the ability to import/export its content. \\
    \hline
    PrivateKey / PublicKey & These classes extend from Key, specialising it for private and public key functionalities respectively. \\
    \hline
    \end{tabular}
\end{table}



\subsection{Implementation}
The approach taken to key generation deviates from standard practices by offering customisable options, particularly in the number of prime factors forming the modulus. Unlike traditional methods that limit the number of primes to two, the code allows the user to specify any number of primes, each with different bit sizes. The modulus and, consequently, the RSA key, are computed based on this specification. This approach aligns with the generalised key generation process outlined in Section \ref{subSec:keygen}. It applies as a consequence of proof implication \ref{IMP:3prime} which requires the use of a non-standard number of primes.  Specifically, the proofs require a 3-prime factor modulus configured as ($\lambda$/4, $\lambda$/4, $\lambda$/2). Under the 2v3PA[$\lambda$] (\ref{def:2v3}) this is deemed to be indistinguishable from the standard two-prime factor modulus (N = pq) thus still acting in alignment with standard RSA.


\begin{lstlisting}[caption=Initialisation of parametrisable Key Generation]
public void initialise(int k, int[] lambda) throws IllegalArgumentException {
    if (lambda.length != k) {
      throw new IllegalArgumentException("Lambda array must have k elements.");
    }

    for (int bitLength : lambda) { 
      keySize += bitLength;
    }

    this.k = k;
    this.lambda = lambda;
  }
 \end{lstlisting}
 
The suitability of Java's BigInteger class as discussed prior becomes evident from the outset. For example when the considering a foundational task: RSA Key Generation and more precisely the generation of the large primes required to encompass the modulus N.
Utilising the constructor BigInteger(int bitLength, int certainty, Random rnd), it is possible to generate probable prime numbers of some arbitrary value, providing a certainty level is indicated.
\begin{lstlisting}[caption=Parametrisable Prime Generation with BigInteger]
public BigInteger[] generatePrimeComponents() {
    BigInteger[] components = new BigInteger[k];
    for (int i = 0; i < k; i++) {
      components[i] = new BigInteger(lambda[i], this.certainty, new SecureRandom());
    }
    return components;
  }
\end{lstlisting}
This leads to the first step to be taken in any implementation of RSA: the initialisation of large prime numbers. 
Due to the magnitude of possible integers required in question, attempting to factorise both p and q to establish their primality with absolute certainty is computationally impractical. Instead, BigInteger employs the Miller-Rabin primality test to assess the probability of primality based on the probability  \[ 1 - \frac{1}{2^{certainty}}\]
For a balance between security and practicality, a certainty value between 50-100 is typical. I chose the midpoint value as a practical compromise.


\begin{lstlisting}[caption= Adaptation for specification of small e]
 private int eBitLength;
 
 public GenRSA(int k, int[] lambda, boolean isSmallE) throws IllegalArgumentException {
    initialise(k, lambda);
    eBitLength = isSmallE ? keySize / 4 : keySize;
  }
  
  public BigInteger computeE(BigInteger phi) {
    BigInteger e = new BigInteger(eBitLength, new SecureRandom());
    while (e.compareTo(ONE) <= 0 || !phi.gcd(e).equals(ONE) || e.compareTo(phi) >= 0) {
      e = new BigInteger(eBitLength, new SecureRandom());
    }
    return e;
  }
\end{lstlisting}

Another novel aspect of this implementation as per implication \ref{IMP:PHI} of the $\varphi$-Hiding Assumption, is the provision for a small public exponent \( e \leqslant N^{1/4} \). This is initiated by providing an additional constructor that includes an isSmallE flag to specify whether a small e is desired. The bit length of e is then calculated accordingly.  If a small \( e \) is chosen, its bit length is set to a quarter of the key size, ensuring compliance as per the considered proof. Otherwise, \( e \)'s bit length matches that of \( \phi \). In either case we have it that \( e \), belongs to the group \( (Z/ Z_{\phi})^{\times} \) meeting standard RSA requirements.

The remaining computation in the attempt to obtain the private exponent d is straightforward with a naturally supported modInverse operation.
\begin{lstlisting}[caption=Java Implementation of Key Generation (\ref{subSec:keygen})]
  
  public KeyPair generateKeyPair() {
    BigInteger[] primes = this.generatePrimeComponents();
    BigInteger N = genModulus(primes);
    BigInteger phi = computePhi(primes);
    BigInteger e = computeE(phi);
    
    BigInteger d = e.modInverse(phi);
    PublicKey publicKey = new PublicKey(N, e);
    PrivateKey privateKey = new PrivateKey(N, primes, phi, e, d);

    return new KeyPair(publicKey, privateKey);
  }

\end{lstlisting}


\section{Design}

\textcolor{red} {For other types of design documentation, please see Appendix B.2.}

This section depicts the structure of classes to be used in the implementation of the project deliverable. For brevity, The section details the design only up to the SignatureController Assembly. The remaining components are detailed at the beginning of Appendix B.2. It can however be noted that the application's modular structure is consistent throughout, so the design principles and patterns applied here for the signature module are similarly reflected across all other modules. 

Embedded within the architecture is a deliberate emphasis on separation of concerns, following the Model-View-Controller (MVC) design pattern. In general terms, MVC separates the application into three interconnected components: the 'Model' for the application's data, the 'View' for the user interface, and the 'Controller' for managing user inputs and updating the Model and View accordingly.
\subsection{Design of Main Controller: Orchestration of Application Flow}


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{main_pictures/mainController.png}
    \caption{Design of Main Controller: Orchestration of Application Flow}
    \label{fig:MAINCONTROLLERDESIGN}
\end{figure}


\subsection{MVC pattern}
At the heart of the design is the MainController, which orchestrates the application flow by responding to user actions and coordinating the display of different views linked to the specialised modules. 

There are two applications of the MVC coordinated by the MainController i.e., the applications two functional (pre-benchmarking) modules: the signatures and additionally the Key Generation module. Most essentially, each of these module's, controller assembly's are encapsulated using a respective mediator class. The role of the mediator classes are to act as an intermediary between the main controller of the application and controller assembly of the relevant functional module. This is required because the application is to consist of various operational modes: i.e, standard, benchmarking, and comparison benchmarking. This class ensures the appropriate controllers are utilised and configured based on the operational context and mode. The mediator pattern fulfils this by obviating the need for the MainController to interact with each controller for an application module individually. The first of these to be considered SignatureMediator.



\subsection{Signature Module}

\textbf{Design of SignatureController Assembly: Orchestration of Signature Related functionalities}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.2\textwidth]{main_pictures/signatureController.png}
    \caption{Design of SignatureController Assembly: Orchestration of Signature Related functionalities}
    \label{fig:SIGCONTROLLERDESIGN}
\end{figure}
Delving deeper into the orchestration of signature-related functionalities, the UML class diagram presents overview of the structure, facilitated by the SignatureMediator class.  
The SignatureMediator serves as a centralised hub acting to manage the flow between the MainController and the application mode specific signature controllers. Its design encompasses concretely the subclasses: SignatureCreationMediator and the SignatureVerificationMediator, each tasked with a specific aspect of signature processing.

The SignatureCreationMediator class is responsible for the initiation and management of signature generation. It interacts with the SignatureCreationControllerStandard (which handles single use non-benchmarked signature creation processes), and with the SignatureCreationControllerBenchmarking for recording and documenting the performance of signature schemes instantiated using differing parameter types on a batch of messages. 
The SignatureVerificationMediator mirrors this structure for the verification processes. 
\begin{comment}
It acts in concert with the SignatureVerificationControllerStandard for singular signature-message pairings and liaises with the SignatureVerificationControllerBenchmarking to handle the benchmarking of verification processes. For example, aiming to record and document the performance of signature schemes instantiated for verifying a provided message-signature batch pairing using differing parameter types.
\end{comment}
Both the SignatureCreationControllerBenchmarking and the SignatureVerificationControllerBenchmarking classes are equipped with a BenchmarkingUtility, which includes a ProgressBar component. This provides live feedback during benchmarking processes, enhancing the user experience by delivering a visual indication of the operation's progress from 0 to 100.

The subclass SignatureCreationControllerComparisonBenchmarking and its verification counterpart, SignatureVerificationControllerComparisonBenchmarking, extend normal benchmarking functionality to accommodate comparison benchmarking. They allow for the side-by-side evaluation of parameter types for a provided signature scheme by key size, offering insights into the relative performance of each signature scheme.

The SignatureModel and SignatureModelBenchmarking classes (see Appendix B.2) serve as a software approximation of a real-world processes of the signing and verification processes. They maintain the state of signature data and respectively handle standard or benchmarking mode operations thus fulfilling the processing of signatures and execution of benchmarking tasks if applicable.

The KeyGenerationMediator mirrors the structure of the SignatureMediator but is focused on generating cryptographic keys through the key generation model assembly which manages the GenRSA facilitated key creation.

\textbf{Observer pattern}

The UML diagram also hints at the observer pattern, a subset of MVC, with the Signature Controllers acting as observers to their relevant Views, which are the observables. When a user interacts with a signature view, such as initiating a signature generation request, the View notifies the SignatureController, which in turn invokes the necessary methods in both the View and the Model to fulfil the request. This decouples the user interface from the business logic, allowing for independent development and refinement of each component.


\section{Implementation: An Extensible Signature Scheme framework}
Having explored the theoretical underpinnings and practical applications of digital signature schemes, we now transition to a more technical focus of how signature schemes can be concretely implemented with a generalised framework.

The foundation of this is provided by SigSchemeInterface, an interface that outlines the essential functionalities required for any digital signature scheme. This interface serves as a contract for implementing classes, ensuring that fundamental operations such as signing, verifying, and data conversion between different formats are standardised and consistent.

\begin{lstlisting}[caption=SigSchemeInterface Interface code] 
public interface SigSchemeInterface {

  byte[] sign(byte[] M) throws SignatureException, DataFormatException;

  boolean verify(byte[] data, byte[] signature) throws SignatureException, DataFormatException;

  BigInteger OS2IP(byte[] EM);

  byte[] I2OSP(BigInteger m) throws IllegalArgumentException;

  BigInteger RSASP1(BigInteger m);

  BigInteger RSAVP1(BigInteger s);
}
 \end{lstlisting}

Signing a message m (sign(m)) works as follows:
\begin{enumerate}
\item Encode the message: encodeMessage(m).

\item Convert the message to representative integer form: OS2IP(m).

\item Using a private key sk, calculate the signature representative s using RSA: RSASP1(sk, m).

\item Convert the signature (integer) representative back to a byte string: I2OSP(s).
\end{enumerate}

Verification is the reverse, adapted accordingly. Given a signature s (verify(s)):

\begin{enumerate}
\item Convert s into an integer representative: OS2IP(s).

\item Using public key vk, calculate the message representative m: RSAVP1(vk, s)

\item Convert m to a byte string: I2OSP(m).

\item Verify the byte string using verifyMessage.
\end{enumerate}

This generalised flow presents two further methods: 'encodeMessage', an algorithm to format data before performing a cryptographic signing operation on its integer representation, and 'verifyMessage', an algorithm that verifies the signature after a cryptographic verification operation has been performed on its integer representation. 
 
Building upon the SigSchemeInterface, the abstract class SigScheme is introduced as a concrete implementation of the Interface for implementing RSA-based signature schemes. This class provides standardised implementations of the interface's methods and 
includes additional functionality specific to RSA operations. It encapsulates key components like the RSA modulus, exponent, and message digest, contributing to a modular and extensible design suitable for various RSA-based signature schemes.

\begin{lstlisting}[caption=Implementation of RSA primitives]

  /**
   * Converts a BigInteger to an octet string of length emLen is the byte length of the RSA modulus.
   */
  public byte[] I2OSP(BigInteger m) throws IllegalArgumentException {
    return ByteArrayConverter.toFixedLengthByteArray(m, this.emLen);
  }

  /**
   * Calculates the RSA signature of a given message representative by raising it to the power of
   * the private exponent as outlined by the RSA algorithm.
   */
  public BigInteger RSASP1(BigInteger m) {
    BigInteger s = m.modPow(this.exponent, this.modulus);
    return s;
  }

  /**
   * Facilitates the verification of RSA signature by raising it to the power of the public exponent
   * as outlined by the RSA algorithm.
   */
  public BigInteger RSAVP1(BigInteger s) {
    return this.RSASP1(s);
  }

}
 \end{lstlisting}
We note that the implementation of RSAVP1 re-uses RSASP1. This design choice is viable because the exponent field in the SigScheme class is generic and can represent either the public or private exponent, depending on the type of key the scheme is instantiated with. This reuse simplifies the code and centralises the modular exponentiation logic in one place.
\newline

\begin{lstlisting}[caption=]
  public BigInteger OS2IP(byte[] EM) {
    return new BigInteger(1, EM);
  }
 \end{lstlisting}
For octet string to integer conversion, the BigInteger(int signum, byte[] magnitude) constructor provides a concise application of the concept succinctly converting the byte array that represents an octet string. The signum is set to 1 (since messages are non-negative) and the byte array of the message acts as the magnitude. The end result is a BigInteger representation of the message, which can then be used in RSA signing or verification processes.

 

\begin{lstlisting}[caption=Signature Scheme specialisation methods]
  // Abstract method to be implemented by derived classes for encoding
  protected abstract byte[] encodeMessage(byte[] M) throws DataFormatException;

  public boolean verifyMessage(byte[] M, byte[] S)
      throws DataFormatException {
    BigInteger s = OS2IP(S);
    BigInteger m = RSAVP1(s);
    byte[] EM;
    try {
      EM = I2OSP(m);
    } catch (IllegalArgumentException e) {
      return false;
    }

    byte[] EMprime = encodeMessage(M);

    return Arrays.equals(EM, EMprime);
  }
 \end{lstlisting}
 
The SigScheme class provides two further methods as compared to the interface with encodeMessage and verifyMessage. The encodeMessage method is left abstract because the encoding of messages is essentially what distinguishes different signature schemes. On the other hand, verifyMessage has a standardised implementation because, in many cases, it simply reverses the signing process. Since the core logic of this reversal often follows a standard procedure that is largely independent of the specifics of the message encoding, it is feasible to provide a standardised implementation for verifyMessage within the SigScheme class.

\subsection*{Message Recovery}

Message recovery schemes, such as ISO/IEC 9796-2 Scheme 1, represent a special case, thereby challenging the notion of standardised implementations for key methods like sign and verifyMessage.
\begin{lstlisting}[caption=Implementation changes for Message Recovery Schemes]
public abstract class SigScheme implements SigSchemeInterface {

  byte[] nonRecoverableM;

  byte[] recoverableM;
}
public class ISO_IEC_9796_2_SCHEME_1 extends SigScheme {
  @Override
  public byte[] sign(byte[] M) throws DataFormatException {
    byte[] S = super.sign(M);
    // Extract m2 from the original message M using the computed m2's length
    if (m2Len > 0) {
      nonRecoverableM = Arrays.copyOfRange(M, m1Len - m2Len, m1Len);
    }
    return S;
  }
}
 \end{lstlisting}
The sign method for these schemes needs to be able to set the non recoverable message field of the class, to allow the respective portion message to be returned to the user, as part of the conclusion to the signature creation process. Hence, the ISO/IEC 9796-2 Scheme 1 class includes an overridden implementation of sign modified accordingly to account for this.

Along these same lines the verifyMessage method which we omit for brevity has to incorporate logic related to the separation of the message into recoverable/non-recoverable parts into the verification process. During verification, the verifyMessage method must correctly reconstruct the original message from these two parts. 

\section{Implementation: Enabling a Provably Secure hash output}
\label{hashOutput} 

This implementation includes support for an unusually large hash output (\(|H()| \ge \frac{\lambda}{2}\)) as required by both proofs. This is accomplished using variable length hash functions like SHAKE-128, or via MGF1 applied to fixed-length functions such as SHA-256.


 \begin{lstlisting}[caption=Retrieval of Message Digest Instances]
public class DigestFactory {
  public static MessageDigest getMessageDigest(DigestType digestType)
      throws NoSuchAlgorithmException, NoSuchProviderException {
    switch (digestType) {
      case SHA_256 -> {return MessageDigest.getInstance("SHA-256");}
      case MGF_1_SHA_256 -> {return MessageDigest.getInstance("SHA-256");}
      case SHA_512 -> {return MessageDigest.getInstance("SHA-512");}
      case MGF_1_SHA_512 -> {return MessageDigest.getInstance("SHA-512");}
      case SHAKE_128 -> {Security.addProvider(new BouncyCastleProvider());
        return MessageDigest.getInstance("SHAKE128", BouncyCastleProvider.PROVIDER_NAME);
      }
      case SHAKE_256 -> {Security.addProvider(new BouncyCastleProvider());
      return MessageDigest.getInstance("SHAKE256", BouncyCastleProvider.PROVIDER_NAME);}
      default -> throw new NoSuchAlgorithmException("Unsupported digest type: " + digestType);
    }
  }
}
 \end{lstlisting}
An enumeration (enum) `DigestType` classifies each hash function. To abstract their creation, a factory pattern is used, which provides a consistent interface to obtain `MessageDigest` objects for each signature scheme. Notably, for SHA-3 family hashes like SHAKE-128 and SHAKE-256, BouncyCastle cryptographic API is used due to the absence of native Java support.

\begin{lstlisting}[caption=Adaptation of Signature Scheme initialisation for specification of provably secure hash output ]
public abstract class SigScheme implements SigSchemeInterface {

  boolean isProvablySecureParams;
  int hashSize;
  
  public SigScheme(Key key, boolean isProvablySecureParams) {
    initialise(key);
    this.isProvablySecureParams = isProvablySecureParams;
  }
  
}
 \end{lstlisting}
To facilitate scheme instantiation with a larger hash output, constructors include an 'isProvablySecureParams' flag. This flag aids in specifying whether an enhanced security hash output is required, allowing the bit length of the hash size to be computed accordingly.



   \begin{lstlisting}[caption=Configuration of Hash Size]
public void setDigest(DigestType digestType, int customHashSize)
      throws NoSuchAlgorithmException, InvalidDigestException, NoSuchProviderException {
    md.reset();
    this.currentHashType = digestType;
    this.md = DigestFactory.getMessageDigest(digestType);
    this.hashID = getHashID(currentHashType);

    if (digestType == DigestType.SHA_256 || digestType == DigestType.SHA_512) {
      this.hashSize = md.getDigestLength(); // Fixed size
    } else if (isProvablySecureParams) {
      this.hashSize = (emLen + 1) / 2; // Provably secure size
    } else {
      if (customHashSize <= 0) {
        throw new IllegalArgumentException("Custom hash size must be a positive integer");
      }
      this.hashSize = customHashSize; // Custom size for variable-length hash types
    }
  }
 \end{lstlisting}

The setDigest method configures the hash size based on the hash type and operational parameters. For fixed size hashes (e.g., SHA-256, SHA-512), it sets the size corresponding to the digest type. For variable-length hashes (e.g., SHAKE-128, SHAKE-256), it offers two options: a preset provably secure size (half the length of the modulus) or a user-defined custom size.


\begin{comment}
This option for a custom size provides extra flexibility since in theory, a provably secure hash output size is not limited to being  half the length of the modulus, and in fact any custom  \textit{Hash output $\displaystyle|H()| \ge \displaystyle\frac{\lambda}{2}$} fulfils the criteria for being provably secure respective to a corresponding key. Additionally it allows for generalised usage of the variable length hash functions within the considered signature schemes thereby facilitating other unrelated use cases and adhering to the principle of scalable design.
\end{comment}

After the hash function is determined and the hash output size configured, marking the final step of the hashing process, the computation of the actual hash within the signature scheme is required.
  \begin{lstlisting}[caption=Final Computation of hash]
public byte[] computeHashWithOptionalMasking(byte[] message) {
    if (!(currentHashType == DigestType.SHA_256 || currentHashType == DigestType.SHA_512)) {
      if (currentHashType == DigestType.MGF_1_SHA_256
          || currentHashType == DigestType.MGF_1_SHA_512) {
        return new MGF1(this.md).generateMask(message, this.hashSize);
      } else {
        return computeShakeHash(message);
      }
    } else {
      return computeHash(message);
    }
  }
 \end{lstlisting}
Finally, the computeHashWithOptionalMasking method calculates the hash within the signature scheme. It handles variable length hash functions distinctively, especially for MGF1, which requires generating a mask using the determined hash size.




\section{Implementation: Application Overview}
The application comprises four main portals, each linked to functionalities essential for facilitating digital signatures. These portals allow transition between standard and benchmarking-specific operations.
\begin{enumerate}
    \item \textbf{Main Menu:} The gateway to key and signature functionalities within the application.
    \item \textbf{Key Generation:} Allows generation of RSA key pairs, either singularly for standard operations or in batches for benchmarking purposes.
    \item \textbf{Signature Creation:} Facilitates creation of individual signatures in standard mode or signature batches in benchmarking mode for deterministic signature types.
    \item \textbf{Signature Verification:} Enables validation of individual or batch signatures against messages or message batches, respectively, in standard or benchmarking modes.
\end{enumerate}


\textbf{Focus on Comparison Benchmarking:}

\begin{comment}
 including Benchmarking (default), Comparison Benchmarking, and Custom Comparison Benchmarking. However this 
\end{comment}
The application offers four modalities, categorised into standard mode and a suite of benchmarking modes. This section focusses on the preset Comparison Benchmarking mode since it most directly aligns with the project's aim to assess the burden on computational time that arises when using provably secure parameters in deterministic RSA signature schemes. 


Comparison Benchmarking mode offers a direct comparative analysis between standard and provably secure key configuration groups (with user selected hash functions for each group). This allows evaluation of the performance of the Standard and Provably Secure groupings, with all trials repeated for each hash function and key configuration within each group. Results are then presented side by side in a table and/or overlaid graphs for each entered key size.

\subsection{Main Menu}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{main_pictures/ui/mainMenu.png}
    \caption{Main Menu}
\end{figure}
The main menu screen of the application presents a straightforward and functional layout, serving as the gateway to its core features via menu buttons. Its design is realised through an FXML layout file, which defines the graphical user interface elements such as buttons. Each button on the main menu is associated with a specific functionality and is observable by the virtue of its MainMenu domain object class. On launch, the MainController registers observers on the observable components of the main menu screen. User interactions prompt the corresponding observer to trigger its effects. The equivalent pattern is followed for all other components in the design of the program. From this point we assume its usage.

\begin{comment}
 For example, the "Generate Keys" button is linked to the following method contained in the class:
\begin{lstlisting}[caption=Mechanism for registering observers]
@FXML private Button generateKeysButton;

void addGenerateKeysObserver(EventHandler<ActionEvent> observer) {
  generateKeysButton.setOnAction(observer);
}
\end{lstlisting}
This mechanism enables the MainController to register observers and observe the main menu, while also underpinning the main menu by setting the primary stage of the application and displaying the MainMenuView as the initial scene.


\begin{lstlisting}[caption=MainController registering an observer to be notified on selection of key generation button]
void showMainMenuView() {
  FXMLLoader loader = new FXMLLoader(getClass().getResource("/MainMenuView.fxml"));
  Parent root = loader.load();
  mainMenuView = loader.getController();
  primaryStage.setScene(new Scene(root));
  primaryStage.show();

  mainMenuView.addGenerateKeysObserver(new GenerateKeysButtonObserver());
  // ...additional observers for other buttons
}
\end{lstlisting}
On launch, the MainController registers observers on the observable components of the main menu screen. User interactions prompt the corresponding observer to trigger its effects.  For example  clicking the "Generate Keys" button, will the trigger GenerateKeysButtonObserver to display of the key generation interface through the KeyGenerationMediator. The mediator serves as an intermediary to the key generation controller assembly. It activates the relevant controller i.e., GenControllerBenchmarking in this case, which in turn presents the user with the appropriate key generation interface in alignment with the system's default state.

\begin{lstlisting}[caption=Implementation of an observer]
public class MainController {
  class GenerateKeysButtonObserver implements EventHandler<ActionEvent> {
    @Override
    public void handle(ActionEvent event) {
      keyGenerationMediator.showBenchmarkingView();
    }
  }
  }
\end{lstlisting}
The equivalent pattern is followed for all other components in the design of the program. From this point we assume its usage.

\end{comment}
\subsection{Key Generation}
\begin{figure}[H]
    \centering % Center the images
    
    % First image in a minipage
    \begin{minipage}{0.495\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/ui/keyGen/keyGen1.png}} % Adding border here
        \caption{Key Generation (number of key sizes)}
        \label{fig:image1}
    \end{minipage}
    \hfill % Add some space between the images
    % Second image in a minipage
    \begin{minipage}{0.495\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/ui/keyGen/keyGen2.png}} % Adding border here
        \caption{Key Generation (key sizes)}
        \label{fig:image2}
    \end{minipage}
    
     \begin{minipage}{0.55\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/ui/keyGen/keyGen3.png}} % Adding border here
        \caption{Key Generation (number of trials)}
        \label{fig:image2}
    \end{minipage}
\end{figure}
The key generation interface for comparison benchmarking is activated through a toggle switch. This interface offers a radio button to select the default comparison benchmarking mode, which compares standard and provably secure key configurations. Users specify the number of key sizes they wish to test, and corresponding input fields are displayed for each size (e.g., Key Size 1: 1024, Key Size 2: 2048).

For each entered key size, the application generates two groups of key configurations:
\begin{itemize}
    \item A \textbf{Standard Parameters} group, with both 2-prime and 3-prime modulus configurations and arbitrary e values.
    \item A \textbf{Provably Secure Parameters} group, with 2-prime and 3-prime modulus configurations and small e values.
\end{itemize}

Users then input the desired number of trials for key generation benchmarking, such as 10 trials. The application executes these trials for each of the four key configurations at the selected key sizes.

\begin{comment}
The GenController class manages the process flow, validating inputs for format and range. Upon confirming the number of trials, the GenModel initiates the benchmarking, with a progress bar indicating real-time completion status.

The initial key generation screen for cross-parameter (comparison) benchmarking is activated by switching on its relevant toggle switch. The first option featured by the screen is a radio button selection for the mode of comparison benchmarking to activate, on  clicking of the submit button. It is the default comparison mode option which facilitates comparison of preset key configuration groupings corresponding to standard and provably secure parameters. After inputting a number corresponding to a desired number of key sizes e.g., 2, a dynamic dialog is displayed with a matching number of key size fields to input individual key sizes e.g., Key Size 1: 1024, Key Size 2: 2048.

This triggers the application on initiation of benchmarking to do the following, for each of the entered key sizes:
\begin{itemize}
\item generate one group of key configurations for \textbf{standard parameters} inclusive of the key configurations for the 2 and 3 prime factor modulus settings i.e., Key 1: $\lambda$/4, $\lambda$/4, $\lambda$/2 \textbf{with arbitrary e} and Key 2: $\lambda$/2, $\lambda$/2 \textbf{with arbitrary e}.
\item generate a second group of key configurations for \textbf{provably secure parameters} inclusive of the key configurations for the 2 and 3 prime factor modulus settings i.e., Key 3: $\lambda$/4, $\lambda$/4, $\lambda$/2 \textbf{with small e} and Key 4: $\lambda$/2, $\lambda$/2 \textbf{with small e}.
\end{itemize}

Subsequently via an updated dialog, the number of  trials intended for the benchmarking of the key generation process can then be entered. In the current scenario with 10 trials, a benchmarking run would consist of running 10 key generations for each of the 4 key configurations for the selected key sizes.

Behind the scenes, the GenController class orchestrates flow between these various stages. Inputs submitted for any stage are subjected to a validation process. This ensures that the provided bit sizes are in an acceptable format and within the operational range for key generation or that more generally submitted inputs are positive integers.
Clicking the "OK" button on the number of trials dialog sets the benchmarking process in motion and engages the GenModel to perform the task. A progress bar is displayed for a length of time spanning the duration of the task allowing for real-time indication of progress.
\end{comment}

\begin{figure}[H]
    \centering
    \includegraphics[scale= 0.325]{main_pictures/ui/keyGen/keyGen4.png}
    \caption{Key Generation (Results Screen)}
\end{figure}
Upon completion of benchmarking, the ResultsController is engaged to display the results from benchmarking. The results screen contains a side pane with buttons corresponding to each individual key size previously entered. Within each side tab, the results table displays a row by row sequence of statistical metrics for all 4 key configurations.  The ordering is by parameter set (i.e., 2 key configurations for standard parameter key results first, followed by the 2 key configurations for provably secure parameter results second). Below the results table, the application also provides the functionality to export the benchmarking results and if desired public or private key batches corresponding to a global set of keys matching the key configurations enumerated in a sequence for all key sizes.




\begin{figure}[H]
    \centering % Center the images
    
    % First image in a minipage
    \begin{minipage}{0.7\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/ui/keyGen/keyGen5.png}} % Adding border here
        \caption{Key Generation: Overlaid Histogram}
        \label{fig:image1}
    \end{minipage}
    \hfill % Add some space between the images
    % Second image in a minipage
    \begin{minipage}{0.7\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/ui/keyGen/keyGen6.png}} % Adding border here
        \caption{Key Generation: Overlaid Box plot graph}
        \label{fig:image2}
    \end{minipage}
     \begin{minipage}{0.7\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/ui/keyGen/keyGen7.png}} % Adding border here
        \caption{Key Generation: Overlaid Line graph for mean times}
        \label{fig:image2}
    \end{minipage}
\end{figure}
An alternative graphical representation of the results can be accessed by navigating to the "Graphs" tab within the central tab pane, where the results table is situated. This tab offers support for three distinct types of graphs, each containing data from all key configurations mapped from the table for easy comparison. These graph types include a stacked histogram, which visualises the distribution of results among configurations for each key size. Additionally, box plots are provided for inferring the distribution of data among configurations. Lastly, mean time line graphs (with error bars for standard deviation) facilitate comparison of average performance and variability among configurations.

\subsection{Signature Generation}
\begin{figure}[H]
    \centering
    \includegraphics[scale= 0.4]{main_pictures/ui/signing/signing1.png}
    \caption{Signature Generation (Comparison Benchmarking)}
\end{figure}
The initial signature generation screen for comparison benchmarking is activated by completing a key generation benchmarking run in comparison mode. On completion of this, the private key batch (which is also available for export from the results table tab) is preloaded into the signature creation controller. Thus when signature generation option is selected from the main menu, the comparison benchmarking screen is displayed rather than the default benchmarking screen.
This explains the state of cross-parameter benchmarking toggle being set as switched on, as well as, specialised options appearing such as the application displaying a visual indication that a comparison-compatible private key batch was preloaded.



\begin{figure}[H]
    \centering % Center the images
    
    % First image in a minipage
    \begin{minipage}{0.4\textwidth}
        \centering
        \fbox{\includegraphics[width=0.4\textwidth]{main_pictures/ui/signing/signing2.png}} % Adding border here
        \caption{Signature Creation (Test message batch (testMessages.txt))}
        \label{fig:image1}
    \end{minipage}
    \hfill % Add some space between the images
    % Second image in a minipage
    \begin{minipage}{0.58\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/ui/signing/signing3.png}} % Adding border here
        \caption{Signature Creation (Imported message batch)}
        \label{fig:image2}
    \end{minipage}
\end{figure}


The signature creation screen is first organised with a pair of fields related to the importing of a message batch. In the backdrop, the SignatureController class waits to be notified of interaction with the message batch import button. The SignatureController expects the message batch as a non interrupted and new line separated sequence of messages. Upon attempt at an import of a message batch, the SignatureController validates that the number of messages in the message batch candidate, matches the number entered in the adjacently above field, and prevents the import in case of a mismatch. On successful validation it updates the screen with a checkmark indicator that provides visual confirmation that the message batch was successfully imported. The interface also includes a dropdown menu labeled signature scheme giving users the option to select the desired signature scheme  to be applied. 

\begin{figure}[H]
    \centering % Center the images
    
    % First image in a minipage
    \begin{minipage}{0.49\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/ui/signing/signing4.png}} % Adding border here
        \caption{Signature Creation (Standard Parameter Hash Functions)}
        \label{fig:image1}
    \end{minipage}
    \hfill % Add some space between the images
    % Second image in a minipage
    \begin{minipage}{0.49\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/ui/signing/signing5.png}} % Adding border here
        \caption{Signature Creation (Provably Secure Parameter Hash Functions)}
        \label{fig:image2}
    \end{minipage}
      \begin{minipage}{0.55\textwidth}
        \centering
        \fbox{\includegraphics[width=1.2\textwidth]{main_pictures/ui/signing/signing6.png}} % Adding border here
        \caption{Signature Creation (All inputs provided)}
        \label{fig:image2}
    \end{minipage}
\end{figure}

In comparison benchmarking mode for signature generation, users select hash functions for standard (fixed-length) and provably secure (variable-length) parameters via a multi-choice drop-down menu. The process involves:

Computing signatures for each selected fixed-length hash function, using the first two key configurations (standard parameters).
Computing signatures for each chosen variable-length hash function, using the last two key configurations (provably secure parameters).This is done sequentially for each key size.

The "Start Signature Benchmarking" button initiates a series of checks and processes. The SignatureController validates the message batch, private key batch, signature scheme, and hash function choices. If any issues are detected, an alert prompts the user to correct them.

Upon passing these checks, the SignatureModel executes the benchmarking task, creating a batch of signatures for the message batch using the designated signature scheme and hash function-key combinations. A progress bar indicates progress in real time.


\begin{comment}
Selection of hash functions for signature generation in comparison benchmarking mode is presented on a per-group basis, with relevant hash function options displayed to enable instantiation of the selected scheme with standard parameters (e.g., fixed-length hash functions) and provably secure parameters (e.g., variable-length hash functions).

The mechanism for selecting the hash function(s) for each group is via a multi-choice drop-down menu. This entails repeating the computation of signatures for the full message set and the first two key configurations (e.g., standard parameter keys) for each of the fixed-length hash functions chosen. Similarly, the computation of signatures for the full message set and the final two key configurations (e.g., provably secure keys) will be repeated for each of the variable-length hash functions selected from the provably secure drop-down menu. This process is enumerated sequentially for each key size.

Upon selection of the "Start Signature Benchmarking" button, a chain of validation and execution steps are executed. The SignatureController first verifies that the message batch input, private key batch, signature scheme and hash function selections are valid and present. Should any discrepancies arise during this stage, the interface promptly presents an alert, informing the user of the necessary corrective measures.


Once the preliminary checks are met, the SignatureModel assumes control to perform the benchmarking task. It computes a batch of  signatures for the provided message batch with the private key batch using the selected signature scheme and relevant hash function to key combinations. A progress bar is displayed for a length of time spanning the duration of the task allowing for real-time indication of progress.
\end{comment}

\begin{figure}[H]
    \centering
    \includegraphics[scale= 0.325]{main_pictures/ui/signing/signing7-2.png}
    \caption{Signature Generation (Results Screen)}
\end{figure}

After completing signature generation benchmarking, the ResultsController displays the results. The results screen features a side pane with tabs for each key size entered. Each tab reveals a results table displaying statistical metrics for all four key configurations.

The table format aligns with the user's hash function choices. For example, selecting one hash function (e.g., SHA-256) for standard parameters results in two rows for this set. For provably secure parameters, if two functions like SHA-256 with MGF1 and SHAKE-128 are chosen, they each generate two rows, totalling four rows. This format leads to six rows per key size, each row representing signature computations for the full message batch.

Below the table, options are available to export benchmarking results and signature batches for each key size. The exported signature batch aligns with the key configurations and hash function combinations used, covering all key sizes in a single file.

\begin{comment}
Upon completion of benchmarking, the ResultsController is engaged to display the results from benchmarking of signature generation. The results screen contains a side pane with buttons corresponding to each individual key size previously entered. Within each side pane tab, the results table displays a row by row sequence of statistical metrics for all 4 key configurations.  

The table format reflects the selection of hash functions by the user for each parameter set. For instance, if one hash function, such as SHA-256 is chosen for the standard parameter set, the table will dedicate 2 rows for each hash function, totalling two rows for the standard parameter set. An identical approach is taken for the provably secure parameter set; if two hash functions like SHA-256 with MGF1 and SHAKE-128 are selected, they will each occupy two rows (because the two provably secure keys would iterate over the full message batch with both hash functions in order) resulting in four additional rows. This arrangement would ensure a presentation of six rows per key size, with each row corresponding to a computation of signatures for the full message batch.

Below the results table, the application provides the functionality to export the benchmarking results for each key size. Additionally signature generation exclusive functionality is provided with an option to export the signature batch corresponding to the global set of keys. The signatures match the key configurations and relevant hash function combinations enumerated sequentially across all key sizes in one file.
\end{comment}


\newpage
\textbf{Signature Generation Results Screen Graphs}

\begin{figure}[H]
    \centering % Center the images
    
    % First image in a minipage
    \begin{minipage}{0.7\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/ui/signing/signing8-2.png}} % Adding border here
        \caption{Signature Generation: Overlaid Histogram}
        \label{fig:image1}
    \end{minipage}
    \hfill % Add some space between the images
    % Second image in a minipage
    \begin{minipage}{0.7\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/ui/signing/signing9-2.png}} % Adding border here
        \caption{Signature Generation: Overlaid Box plot graph}
        \label{fig:image2}
    \end{minipage}
        \begin{minipage}{0.7\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/ui/signing/signing10-2.png}} % Adding border here
        \caption{Signature Generation: Overlaid Line graph for mean times}
        \label{fig:image2}
    \end{minipage}
\end{figure}

Signature generation results can also be visualised graphically, accessible under the "Graphs" tab in the central pane. These graphs, correlating with key configurations from the results table, offer an alternate method for interpreting data.

\begin{comment}
Another way to interpret signature generation results is visually through graphical representation and this aligns with key configurations from the respective results table. The alternative view can be found under the "Graphs" tab, located in the central tab pane where the results table is placed. Accessing and interpreting these graphs follows the same procedure as those created during the benchmarking of key generation. 
\end{comment}
\subsection{Signature Verification}

\begin{figure}[H]
    \centering
    \includegraphics[scale= 0.4]{main_pictures/ui/verifying/verifying0.png}
    \caption{Signature Verification (Comparison Benchmarking)}
\end{figure}

Similar to the process for signature generation, the signature verification screen for comparison benchmarking is accessible after completing a key generation benchmark in comparison mode. This procedure also includes preloading a batch of keys. However, the key distinction lies in the type of key batch loaded. In the case of signature verification, it's a public key batch that gets preloaded, as opposed to a private key batch used for signature generation.


\begin{figure}[H]
    \centering % Center the images
    
    % First image in a minipage
    \begin{minipage}{0.4\textwidth}
        \centering
        \fbox{\includegraphics[width=0.4\textwidth]{main_pictures/ui/signing/signing2.png}} % Adding border here
        \caption{Signature Verification (Test message batch (testMessages.txt))}
        \label{fig:image1}
    \end{minipage}
    \hfill % Add some space between the images
    % Second image in a minipage
    \begin{minipage}{0.58\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/ui/verifying/verifying2.png}} % Adding border here
        \caption{Signature Verification (Imported message batch)}
        \label{fig:image2}
    \end{minipage}
\end{figure}

Aside from the preloaded public key batch used for comparison benchmarking, the signature verification screen includes fields for importing both message and signature batches. For correctness, it's necessary that the submitted message batch aligns with the submitted signature batch. Moreover, these batches must be compatible with the public key batch, which, in turn, is associated with the private key batch employed during the prior signature generation phase.


\begin{figure}[H]
    \centering % Center the images
    
    % First image in a minipage
    \begin{minipage}{0.49\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/ui/verifying/verifying3.png}} % Adding border here
        \caption{Signature Verification (Standard Parameter Hash Functions)}
        \label{fig:image1}
    \end{minipage}
    \hfill % Add some space between the images
    % Second image in a minipage
    \begin{minipage}{0.49\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/ui/verifying/verifying4.png}} % Adding border here
        \caption{Signature Verification (Provably Secure Parameter Hash Functions)}
        \label{fig:image2}
    \end{minipage}
\end{figure}


In signature verification (comparison) benchmarking, hash function selection must align with those used in signature generation to ensure correctness. The process starts with the user choosing matching hash functions for each parameter type. This is vital because the hash functions used must correspond to the message-signature pairs created in the signature generation phase, which used the private key batch related to the currently imported public key batch.

Upon selecting "Start Verification Benchmarking," the SignatureController validates the number of signatures against the message batch and chosen hash functions. If there's a mismatch, an alert notifies the user of the discrepancy. Following successful validation, the SignatureModel takes over to execute the verification benchmarking. This involves comparing the signature batch with the message batch and public keys, using the selected signature scheme and hash functions. The process generates batches of verification results for each entered key size, and a progress bar provides real-time progress updates.


\begin{comment}
Hash function selection for signature verification in comparison benchmarking mode mirrors the mechanism used for signature creation. For correctness,  the selected hash functions for each parameter type must match those submitted for the equivalent message-signature batch pairing generated during the prior signature generation process that contained the private key batch  corresponding to the currently imported public key batch.

Upon selection of the "Start Verification Benchmarking" button, a chain of validation and execution steps are executed. The SignatureController first verifies that the number of signatures in the signature batch matches the expected based on the number of messages and selected hash function choices. Should a discrepancy arise during this stage with a mismatch between the number of signatures and message batch/hash function combination, the interface promptly presents an alert, informing the user of the necessary corrective measures.

Once the preliminary checks are met, the SignatureModel assumes control to execute the benchmarking task for verification. It compares the provided signature batch against the message batch with the public key batch using the selected signature scheme and relevant hash function to key combinations. A byproduct of the process is the generation of batches of verification results corresponding to each previously entered key size, while a progress bar is displayed for a length of time spanning the duration of the task allowing for real-time indication of progress.
\end{comment}



\newpage
\textbf{Signature Verification Results Screen}

\begin{figure}[H]
    \centering
    \includegraphics[scale= 0.325]{main_pictures/ui/verifying/verifying6.png}
    \caption{Signature Verification (Results Screen)}
\end{figure}

Once the signature verification benchmarking concludes, the ResultsController takes over to showcase the obtained results. The layout of this results screen is largely as was shown for signature generation. The distinction is the enabling of the export of verification results for each key size. displayed below each results table.

\begin{figure}[H]
    \centering % Center the images
    
    % First image in a minipage
    \begin{minipage}{0.7\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/ui/verifying/verifying7.png}} % Adding border here
        \caption{Signature Verification: Overlaid Histogram}
        \label{fig:image1}
    \end{minipage}
    \hfill % Add some space between the images
    % Second image in a minipage
    \begin{minipage}{0.7\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/ui/verifying/verifying8.png}} % Adding border here
        \caption{Signature Verification: Overlaid Box plot graph}
        \label{fig:image2} 
    \end{minipage}
     \begin{minipage}{0.7\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/ui/verifying/verifying9.png}} % Adding border here
        \caption{Signature Verification: Overlaid Line graph for mean times}
        \label{fig:image2}
    \end{minipage}
\end{figure}

\subsection{Signature Processing for Message Recovery Schemes}

In the Signature processes, when utilising the message recovery ISO/IEC 9796-2 Scheme, there is a specific approach for handling non-recoverable portion when signature generation is performed and recovered portion of messages when signature verification is performed.

\begin{figure}[H]
    \centering % Center the images
    
    % First image in a minipage
    \begin{minipage}{0.2\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/ui/signing/signing2.png}} % Adding border here
        \caption{Signature Creation (Test message batch (testMessages.txt))}

        \label{fig:image1}
    \end{minipage}
    \hfill % Add some space between the images
    % Second image in a minipage
    \begin{minipage}{0.7\textwidth}
        \centering
         \fbox{\includegraphics[width=\textwidth]{main_pictures/ui/signing/signing11.png}} % Adding border here
        \caption{Signature Generation with ISO scheme selected}        \label{fig:image2}
    \end{minipage}
    
      \begin{minipage}{0.7\textwidth}
        \centering
         \fbox{\includegraphics[width=\textwidth]{main_pictures/ui/signing/signing12.png}} % Adding border here
        \caption{Signature Generation Results Screen (with Message Recovery Options)}        \label{fig:image2}
    \end{minipage}
\end{figure}

For signature generation this process involves exporting a batch of non-recoverable messages where each line corresponds with a signature. Lines are prefixed with a "1" to indicate the presence of a non-recoverable message and "0" when there is none. In this specific case, since all messages are too short to produce a non-recoverable part regardless of the key sizes selected, the resulting file comprises a sequence of "0" flags, each separated by a new line.


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{main_pictures/ui/verifying/verifying10.png}
    \caption{Signature verification with ISO scheme selected}
\end{figure}

The corresponding verification of signatures necessitates import of a non-recoverable message batch file generated from the preceding signature generation benchmarking process. As discussed it contains entries aligned with each signature, organised line by line where each entry starts with a "1" or "0" flag, indicating whether a non-recoverable message part is present or absent, respectively. The application then leverages these non-recoverable message parts, to construct the content of a recovered message batch. This information is then displayed as a dedicated column in the verificationResults CSV files, which users can export for further analysis.


\textcolor{red} {For insight into the testing process see Appendix B.3 for more details.}
\begin{comment} (the the basis for many other de facto standards including ANSI X9, IETF, TLS) t \end{comment}
\chapter{Professional Issues: Adherence to Standards in Cryptography}



A standard is an established set of guidelines or specifications that provide recommendations for processes, practices, or systems. They are internationally agreed ways of working make technology more secure and interoperable \cite{isoCryptography2024}. In fact the most common mechanism for determining an appropriate cryptographic primitive, scheme or parameter choice is to consult a standard. 

The evolution of cryptography from military to commercial applications created a demand for standardisation in the area. There are thus an extensive range of international standards related to cryptography.  Of most pertinence to the signature schemes considered in this project we have the following three:
\begin{itemize}

\item \textit{\textbf{PKCS\# 1 (\cite{rfc8017})}} 
foundational RSA standard that outlines the mathematical properties and structure of RSA public and private keys. It also defines basic algorithms, encoding methods, and padding schemes necessary for RSA primitives.
\item \textit{\textbf{ANSI X9.31 (\cite{ANSI-1998-X9-31})}} is a now withdrawn United States financial industry standard for digital signatures based on the RSA algorithm.
\item \textit{\textbf{ISO/IEC 9796-2 (\cite{ISO/2010/9796-2-2010})}} defines three different RSA signature schemes that support message recovery including Digital Signature 1 which is used in the EMV payment system (provides a “platform” for enabling a chip card to be used in banking applications).
\end{itemize}

The demand for standards in cryptography has supplanted the traditional practice of security through obscurity, where keeping algorithmic details secret was the norm. Today, these details are open to the public, inviting scrutiny and evaluation from the international community of cryptographic researchers. This open approach assures that by the time an algorithm is integrated into a standard, it has withstood the rigours of peer evaluation, instilling confidence in developers who rely on these standards for secure application development.

Standards distill theoretical cryptographic techniques into clear guidelines covering implementation, appropriate contexts for use, and selection of algorithm parameters. They effectively bridge the gap between cryptographers, who typically have a deep understanding of mathematical theory but may lack practical software development experience, and software developers, who often lack extensive knowledge in cryptography.

This delineation of responsibilities ensures that each group can focus on their area of expertise while relying on shared, vetted practices. Standards in cryptography thus serve as a common language, enabling developers to adopt secure algorithms without needing to understand their theoretical underpinnings fully.


The world of cryptography offers numerous instances where implementations found not to follow standards correctly led to critical vulnerabilities \cite{4223237, 10.1145/2382196.2382205, 10.1145/2382196.2382204,  bleichenbacher1998chosen}. Among these, the Bleichenbacher signature forgery attacks \cite{finney2006bleichenbacher, kuhn2008variants} stands out as a prime example. This class of attacks, which worked on vulnerable implementations of the PKCS\#1 signature scheme, showcased the potential for serious security breaches with. A real-world example targeting Facebook  \cite{bock2018return} in 2018 underscored the risk and during the same period, had been found to be affecting almost a third of the top 100 domains in the Alexa Top 1 Million list. Going further, such was severity of the threat, that a server could potentially be impersonated without accessing its private key. This starkly demonstrate how deviations from standards can lead to severe security breaches, and emphasises how it should be professional responsibility to implement these standards correctly.


In the context of this project, a key consideration was adhering to the standards defining the implemented signature schemes. It was paramount, not only to ensure that my initial implementations of these schemes conformed to the respective standards delineating them, but also to ensure that any modifications made for incorporating provably secure parameters aligned with established standards.

For example considering the latter, the first change included implementing  signatures to use RSA keys with a non-standard number of primes (three instead of two), which the PKCS\#1 standard already readily accommodates by defining the modulus N as a product of two or more primes. Another adjustment was the use of a large hash size ($\geq \lambda/2$). Although not directly addressed in the three standards for use in the signature schemes, standardised solutions are available as per the use of variable length hash functions for other use cases.  I adopted two of the most established methods for this: one being the MGF1 construction detailed in the PKCS\#1 standard \cite{rfc8017} and the other being the Keccak-based SHAKE hash functions (standardised by NIST in FIPS 202 \cite{1421}), both of which allow for variable length output. These adaptations ensured the project stayed within the bounds of established cryptographic standards. 

Furthermore, it was important for me that alignment with standards was to be achieved in a manner that was both ethical and practical. Since cryptographic algorithms are almost always necessary for securing sensitive data that might be communicated or stored, implementation of cryptographic standards, therefore, transcends mere technical expertise. It encompasses a broader professional and ethical responsibility towards safeguarding data and protecting privacy. This responsibility becomes increasingly significant in the current era where data breaches and cyberattacks are commonplace. Thus the approach of attempting to adhere to cryptographic standards I did not view simply a professional mandate but also as an ethical imperative to uphold the integrity and security of digital systems. This holds even in the context of my project which while academic, strived to emulate real-world conditions and implement actual signature schemes.
Throughout the project, I was cognisant of these factors and took steps to ensure that my implementations and modifications did not stray significantly from the path charted by the standards.

One example of a hurdle I faced was the foremost one: obtaining the required documentation for the standards related to schemes. Specifically, accessing ANSI X9.31 and ISO/IEC 9796-2 standards was difficult as they are not publicly available, and ISO/IEC standards, in general, are often not freely accessible and can be costly.

Fortunately, the BSI website offers free access to ISO/IEC standards for students at institutions with subscriptions. As Royal Holloway has such a subscription \cite{BSIWebsite}, I was able to obtain the ISO/IEC 9796-2 standard. Meanwhile, the PKCS\#1 standard was readily available as it is published as part of the Internet Engineering Task Force's (IETF) RFC (Request for comments) series, specifically as RFC 8017 \cite{rfc8017}.

Addressing the issue with ANSI X9.31, which is a deprecated standard and hence not publicly available, required a different approach. I turned to peer-reviewed academic research papers that detailed the ANSI X9.31 implementation \cite{10.1007/978-3-030-64357-7_5}. These papers, while providing more of a high-level overview, aided me building in understanding the scheme. This then allowed me to leverage the similarities between ANSI X9.31 and PKCS\#1, both being variants of the signature schemes with appendix class. Using insights provided by the PKCS\#1 standard, I was able to address the implementation details that the academic papers did not fully cover. Furthermore the PKCS\#1 standard provided a reference for updating the implementation of the ANSI X9.31 scheme to reflect current cryptographic practices not available at the time the ANSI X9.31 standard was devised. 








\chapter{Results and Discussion}

\section{Setup and Methodology}
In order to compare the timing differential between standard and provably secure parameters, multiple disparate benchmarking runs were initiated in comparison benchmarking mode through the key generation portal for 6 key sizes increasing in 1024-bit segments from 1024-bit to 6144-bit. The number of trials for benchmarking was set to 3847 (which is the control variable used for every key configuration across key generation and signature creation/verification) to observe the behaviour of time taken. This specific number is related to the number of trials (messages) in the message batch that was used in benchmarking of the signature schemes.

\subsection{Dataset}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{main_pictures/ui/dataset.png}
    \caption{Example of Oxford 3000 Dataset being imported for benchmarking}
\end{figure}

The dataset employed for the benchmarking of Signature Creation and Verification was derived from the first edition of the Oxford 3000 (2017) \cite{Oxford3000_2022}, a collection of the 3847 most important words to learn in English, as defined by the Oxford University Press. This dataset was utilised as the message batch taken from a text file representation of the list \cite{Oxford3000_GitHub} publicly available on GitHub. The aim was for each word/line from the Oxford 3000 list to serve as an individual message for processing through the signature schemes. The size of this dataset, with its 3847 entries, served as the control variable applied for every key configuration as its number of trials. 

\begin{comment}
By using this dataset, the benchmarking aimed to replicate realistic scenarios where such commonly used words might be part of digital communications that require digital signature operations. The size of the Oxford 3000 was appropriately large enough to provide substantial data for processing, yet manageable enough for a benchmarking exercise, helping to find balance to ensure that the benchmarking was comprehensive but not overly time-consuming or resource-intensive.
\end{comment}


\subsection{Hash function selection}

\begin{figure}[H]
    \centering 
    \includegraphics[width=\textwidth]{main_pictures/ui/hashChoices.png}
    \caption{Hash function selections respective to each parameter type used for benchmarking}
\end{figure}

\begin{comment}
For purposes of the comparative benchmarking, SHA-256 was selected as the hash function for instantiation of the schemes with standard parameters. Endorsed by the National Institute of Standards and Technology (NIST) as the minimum acceptable standard, SHA-256 is the most commonly utilised hash function thanks to its balance of performance and security. These reasons make it apt for representing standard instantiation of the considered signature schemes. 

For the instantiation of schemes with provably secure parameters, all variable-length hash functions available in the application were selected. This choice aimed to ensure that all types of provably secure instantiations could be accounted for in the subsequent analysis and allow deeper inspection into the implications of using provably secure parameters.
\end{comment}


\subsection{Hardware Specifications}
The following hardware specifications outline the computing environment used for all benchmarking exercises:
\begin{itemize}
\begin{comment} \item \textbf{Model:} Macbook Pro 14' (2021) \end{comment}
\item \textbf{Operating System:} macOS Sonoma 14.3.1
\item \textbf{CPU:} Apple M1 Pro
\item \textbf{Total Number of Cores:} 10 (8 performance and 2 efficiency)
\item \textbf{Clock Speed:} 3220 MHz (and 2064 MHz for Efficiency Cores)
\item \textbf{Instruction set:} ARMv8-A64 (64 bit)
\item \textbf{Memory:} 32 GB (LPDDR5 RAM)
\end{itemize}
\begin{comment}
The selected hardware represents a modern computing environment. Therefore, the results obtained from the benchmarking are indicative of the performance one might expect in similar modern computational settings.
\end{comment}
It's important to note, that the considered deterministic schemes, often operate in environments vastly different from this testing setup which constitutes a modern computing environment. These include constrained legacy devices or low-performance environments, such as those found in payment/banking systems (e.g., EMV chip-and-pin cards) and secure network protocols.  Consequently benchmarking results may not be fully applicable or generalisable to the diverse range of use cases where these older schemes are still operational.  

\begin{comment}
While the benchmarking results provide valuable insights into the performance of cryptographic operations on modern hardware, they may not be fully applicable or generalisable to the diverse range of use cases where these older schemes are still operational. Therefore, caution should be exercised when interpreting these results outside the specific context of the testing environment.
\end{comment}

\section{Results}
The results of the timing, using the developed application for key generation and subsequently and signature creation/verification for each deterministic signature scheme is summarised in tables. 

\textit{*For the views of the complete result tables and subsequent breakdown by parameter type, specific to the ANSI X9.31 rDSA and ISO/IEC 9796-2:2010 Signature Schemes, please consult Appendix D.}

\section{Key Generation Results}

\begin{figure}[H]

    \centering % Center the images
     \caption{Standard vs Provably Secure Key Configurations for 1024-bit Key Size}
    % First image in a minipage
    \begin{minipage}{\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/keyGen/keyGen_1024bit_table1_1.png}} 
        \fbox{\includegraphics[width=\textwidth]{main_pictures/keyGen/keyGen_1024bit_table2_1.png}}
        \label{fig:image1}
    \end{minipage}
        \label{keyGen_1024bit_table}
  \end{figure}
  
\begin{figure}[H]
  
    \centering % Center the images
     \caption{Standard vs Provably Secure Key Configurations for 2048-bit Key Size}
    % First image in a minipage
    \begin{minipage}{\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/keyGen/keyGen_2048bit_table1_1.png}} 
        \fbox{\includegraphics[width=\textwidth]{main_pictures/keyGen/keyGen_2048bit_table2_1.png}}
        \label{fig:image1}
    \end{minipage}
      \label{keyGen_2048bit_table}
  \end{figure}
  
\begin{figure}[H]

    \centering % Center the images
     \caption{Standard vs Provably Secure Key Configurations for 3072-bit Key Size}
    % First image in a minipage
    \begin{minipage}{\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/keyGen/keyGen_3072bit_table1_1.png}} 
        \fbox{\includegraphics[width=\textwidth]{main_pictures/keyGen/keyGen_3072bit_table2_1.png}}
        \label{fig:image1}
    \end{minipage}
     \label{keyGen_3072bit_table}
\end{figure}

\begin{figure}[H]

    \centering % Center the images
     \caption{Standard vs Provably Secure Key Configurations for 4096-bit Key Size}
    % First image in a minipage
    \begin{minipage}{\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/keyGen/keyGen_4096bit_table1_1.png}} 
        \fbox{\includegraphics[width=\textwidth]{main_pictures/keyGen/keyGen_4096bit_table2_1.png}}
        \label{fig:image1}
    \end{minipage}
         \label{keyGen_4096bit_table}
\end{figure}

\begin{figure}[H]

    \centering % Center the images
     \caption{Standard vs Provably Secure Key Configurations for 5120-bit Key Size}
    % First image in a minipage
    \begin{minipage}{\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/keyGen/keyGen_5120bit_table1_1.png}} 
        \fbox{\includegraphics[width=\textwidth]{main_pictures/keyGen/keyGen_5120bit_table2_1.png}}
        \label{fig:image1}
    \end{minipage}
         \label{keyGen_5120bit_table}
\end{figure}

\begin{figure}[H]

    \centering % Center the images
     \caption{Standard vs Provably Secure Key Configurations for 6144-bit Key Size}
    % First image in a minipage
    \begin{minipage}{\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/keyGen/keyGen_6144bit_table1_1.png}} 
        \fbox{\includegraphics[width=\textwidth]{main_pictures/keyGen/keyGen_6144bit_table2_1.png}}
        \label{fig:image1}
    \end{minipage}
        \label{keyGen_6144bit_table}
\end{figure}



Tables \ref{keyGen_1024bit_table} to \ref{keyGen_6144bit_table} show timing statistics in milliseconds (correct to 5 decimal places) for both standard and provably secure key configurations across a range of key sizes, incrementing by 1024 bits from 1024 to 6144 bits. 


\textbf{Comparing 2 vs 3 primes within parameter groups}

Table \ref{tab:2vs3primes_percentage} illustrates the percentage difference in mean generation times when moving from 2-prime to 3-prime configurations for each parameter group. 

\begin{table}[H]
\centering
\caption{Efficiency Comparison of 2 vs 3 Primes in Key Generation (Percentage Difference)}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Key Size (bits)} & \textbf{2 Primes to 3 Primes \% Diff} & \textbf{2 Primes to 3 Primes \% Diff} \\
 & \textbf{(Standard Parameters)} & \textbf{(Provably Secure Parameters)} \\
\hline
1024 & -20.07\% & -24.43\% \\
2048 & -34.17\% & -33.54\% \\
3072 & -39.22\% & -37.30\% \\
4096 & -39.30\% & -39.23\% \\
5120 & -39.16\% & -40.20\% \\
6144 & -40.77\% & -40.85\% \\
\hline
\end{tabular}
\label{tab:2vs3primes_percentage}
\end{table}

A consistent trend observed across all key sizes from 1024 to 6144 bits is the superior performance of 3-prime configurations over 2-prime configurations. This pattern holds true within both standard and provably secure parameter types. Notably, the efficiency advantage of 3-prime configurations also increased with key size.


\textbf{Comparing Standard vs provably secure key configurations}

The distinction of the pattern being within parameter types is an important one to make because it illustrates what appears to be an inherent efficiency gain in key generation when additional prime factors are used, irrespective of whether the parameters are standard or provably secure. For example, at the 1024-bit key size, while both provably secure configurations with 2 and 3 primes show a slight performance increase over their standard counterparts, comparing across different parameter types with a different number of primes (like 2 primes provably secure vs. 3 primes standard) is not equitable due to the fundamental difference in the configuration setup.

More concretely, at the 1024-bit key size \ref{keyGen_1024bit_table}, provably secure configurations with both 2 and 3 primes show modest efficiency gains over standard counterparts. Specifically, the provably secure 2-prime configuration was approximately 2.04\% quicker (8.27606 ms compared to 8.44815 ms), and the provably secure 3-prime configurations demonstrated a 7.35\% reduction in mean time (6.25438 ms versus 6.75122 ms). This trend challenges the usual belief that enhanced security compromises performance. Both configuration types exhibit relatively lower standard deviation and variance at this size, indicating consistent performance.

As key sizes increase, to 2048 bits \ref{keyGen_2048bit_table}, the pattern of improved efficiency in provably secure configurations compared to standard holds, with about 2\% faster mean time in the  2-prime configuration and 1.4\% in 3-prime configuration. However, at 3072 bits  \ref{keyGen_3072bit_table}, a deviation emerges: while the 3-prime provably secure configuration maintained a 12.3\% efficiency advantage, the  2-prime provably secure configuration bucked the trend coming in at 8.91\% slower than its standard counterpart. This size also marked a notable increase in standard deviation and variance, a suggestion of a wider spread in key generation times.

Moving to 4096-bit  and 5120-bit key sizes, provably secure configurations maintain their lead in efficiency over standard configurations, although the margin gets narrower. 

\begin{comment}
Specifically, at 4096 bits \ref{keyGen_4096bit_table}, the mean time for generating a key with a provably secure 2-prime configuration was marginally faster by 0.70\% (648.50270 ms) than the standard setup (653.09737 ms). The advantage is even slighter in the 3-prime category, with a 0.58\% quicker generation time (394.08127 ms for provably secure versus 396.39114 ms for standard).
At the 5120-bit Key Size \ref{keyGen_5120bit_table}, the provably secure 2-prime configuration was 1.38\% faster (1335.99449 ms compared to 1345.80116 ms for standard), while the 3-prime configuration was 0.96\% faster (805.03730 ms compared to 812.86892 ms for standard). The progressive deterioration in standard deviation and variance continued, hinting at greater variability and a potential overshadowing of the small efficiency gains.
 \end{comment}
 
At the largest tested size of 6144 bits \ref{keyGen_6144bit_table}, the efficiency gap remains minimal, with provably secure parameters showing only about 0.49\% and 0.64\% improvements for 2-prime and 3-prime configurations respectively. This key size experienced the highest standard deviation and variance, emphasising significant variability in key generation times.


\subsection*{Summary}
Throughout the analysis, as expected, the time taken for key generation increases with the key size. However, the increase is not linear; it tends to grow more than proportionally, reflecting the greater computational complexity involved in generating and handling larger primes. Particularly notable in this context is the significant escalation in standard deviation and variance at larger key sizes for both standard and provably secure configurations. 

While provably secure parameters exhibit some efficiency improvements, these gains are relatively minimal, especially as key sizes grow larger. The increased variability at larger key sizes, as indicated by higher standard deviation and variance, obscures the true impact of opting for provably secure parameters since the small percentage difference may fall within the broader range of natural variability of the key generation process. Consequently, although provably secure configurations show efficiency improvements at smaller key sizes, their practical significance wanes with increasing key sizes.

In essence there is no significant difference between key generation using standard versus provably secure configurations and the distinction between the two becomes negligible, with increasingly larger key sizes. This observation challenges the expectation that using provably secure parameters would impede performance. It implies that adopting provably secure key configurations might not only be a security upgrade but also a practically viable choice that in some cases offers an improvement in performance.


\section{Signature Creation Results}
\subsection{Signature Creation Results (PKCS\#1 v1.5)}

\begin{figure}[H]
    \centering % Center the images
     \caption{Instantiation of PKCS\#1 v1.5 with standard vs provably secure parameters (1024-bit Key Size) for signature creation}
    % First image in a minipage
    \begin{minipage}{\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_sign_1024bit_table1_1.png}} 
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_sign_1024bit_table2_1.png}}
    \end{minipage}
            \label{pkcs_sign_1024bit_table}
  \end{figure}
  
\begin{figure}[H]
    \centering % Center the images
     \caption{Instantiation of PKCS\#1 v1.5 with standard vs provably secure parameters (2048-bit Key Size) for signature creation}
    % First image in a minipage
    \begin{minipage}{\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_sign_2048bit_table1_1.png}} 
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_sign_2048bit_table2_1.png}}
    \end{minipage}
            \label{pkcs_sign_2048bit_table}
  \end{figure}
  
\begin{figure}[H]
    \centering % Center the images
     \caption{Instantiation of PKCS\#1 v1.5 with standard vs provably secure parameters (3072-bit Key Size) for signature creation}
    % First image in a minipage
    \begin{minipage}{\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_sign_3072bit_table1_1.png}} 
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_sign_3072bit_table2_1.png}}
    \end{minipage}
         \label{pkcs_sign_3072bit_table}
\end{figure}

\begin{figure}[H]
    \centering % Center the images
     \caption{Instantiation of PKCS\#1 v1.5 with standard vs provably secure parameters (4096-bit Key Size) for signature creation}
    % First image in a minipage
    \begin{minipage}{\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_sign_4096bit_table1_1.png}} 
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_sign_4096bit_table2_1.png}}
    \end{minipage}
             \label{pkcs_sign_4096bit_table}
\end{figure}

\begin{figure}[H]
    \centering % Center the images
     \caption{Instantiation of PKCS\#1 v1.5 with standard vs provably secure parameters (5120-bit Key Size) for signature creation}
    % First image in a minipage
    \begin{minipage}{\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_sign_5120bit_table1_1.png}} 
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_sign_5120bit_table2_1.png}}
    \end{minipage}
     \label{pkcs_sign_5120bit_table}
\end{figure}

\begin{figure}[H]
    \centering % Center the images
     \caption{Instantiation of PKCS\#1 v1.5 with standard vs provably secure parameters (6144-bit Key Size) for signature creation}
    % First image in a minipage
    \begin{minipage}{\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_sign_6144bit_table1_1.png}} 
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_sign_6144bit_table2_1.png}}
    \end{minipage}
         \label{pkcs_sign_6144bit_table}
\end{figure}
\begin{comment}
\textbf{1024-bit Key Size (\ref{pkcs_sign_1024bit_table})}:

For the 1024-bit key size, examining both standard and provably secure parameters reveals subtle differences. With standard parameters using two primes, the mean time for SHA-256 is 30.71927 ms. However, when comparing this with provably secure parameters with two primes across its various hash functions, the mean times range from 29.96431 ms to 30.22801 ms. This indicates a performance variation of up to approximately 2.46\%, either as a decrease or an increase, depending on the specific hash function and configuration. Similarly, with three primes, standard parameters for SHA-256 show a mean time of 30.09907 ms, whereas provably secure parameters exhibit a mean time range from 30.06791 ms to 30.22801 ms, reflecting a potential performance variation of up to around 0.43\%.

\textbf{2048-bit Key Size (\ref{pkcs_sign_2048bit_table})}:

With standard parameters and 2 primes, the mean time is 30.12582 ms. Comparing this to the provably secure parameters with 2 primes, the range is from 30.17380 ms to 30.46944 ms, showing an increase in time by up to 1.14\%. In the case of 3 primes, the mean time for standard parameters is 30.17957 ms. Provable parameters with 3 primes fluctuate from 30.19581 ms to 30.43499 ms, showing a slight performance decrease by up to 0.85\%.

\textbf{3072-bit Key Size (\ref{pkcs_sign_3072bit_table})}:

For standard parameters with 2 primes, the mean is 30.59674 ms. In contrast, provable parameters with 2 primes vary between 30.27461 ms and 30.45424 ms, representing an improvement of up to 1.05\% or a decrease of up to 0.3\%. The standard parameters with 3 primes show a mean time of 30.45630 ms, whereas the provable parameters are between 30.29737 ms and 30.32374 ms, implying very negligible changes in performance, with a decrease of up to 0.52\%.

\textbf{4096-bit Key Size (\ref{pkcs_sign_4096bit_table})}:

The standard 2 prime parameters yield a mean time of 30.27737 ms, while the provable parameters show a mean time range from 30.20630 ms to 30.45424 ms, reflecting a slight improvement of up to 0.24\% or an increase of up to 0.58\%. For 3 primes, the standard parameters stand at 30.05734 ms mean time, and the provable parameters range from 30.16106 ms to 30.32374 ms, with a potential increase in time up to 0.89\%.

\textbf{5120-bit Key Size (\ref{pkcs_sign_5120bit_table})}:

Standard 2 prime parameters give a mean time of 30.15069 ms. The provable parameters with 2 primes range from 30.03234 ms to 30.23318 ms, indicating an improvement of up to 0.39\% or a slight increase of up to 0.27\%. With 3 primes, the standard parameters show a mean of 30.06262 ms, compared to provable parameters ranging from 30.09692 ms to 30.13618 ms, suggesting a minimal performance change.

\textbf{6144-bit Key Size (\ref{pkcs_sign_6144bit_table})}:

Standard parameters with 2 primes have a mean time of 30.35987 ms, whereas the provable parameters range from 30.19886 ms to 30.45424 ms, showing an improvement of up to 0.53\% or an increase of up to 0.31\%. With 3 primes, the standard mean time is 30.12852 ms, and the provable parameter mean times vary from 30.20596 ms to 30.32374 ms, presenting a marginal performance decrease up to 0.65\%.
\end{comment}

\subsection*{Summary}


Across all key sizes, PKCS\#1 v1.5 benchmarking results for signature creation demonstrate subtle variations between standard and provably secure parameters. For SHA-256 with standard parameters using two primes, mean times range from 30.71927 ms to 30.35987 ms. For provably secure parameters with two primes, across different hash functions, the mean times vary from 29.96431 ms to 30.45424 ms, indicating a performance variation up to approximately 2.46\%. Using three primes, standard parameters yield mean times from 30.09907 ms to 30.12852 ms, while for provably secure parameters, the range is from 30.06791 ms to 30.32374 ms, suggesting a potential variation of up to 0.65\% across all key sizes. 

Overall, the fluctuations in mean times across different configurations and hash functions are relatively minor. Given the scale of the operation and the small magnitude of change, these variations could likely be attributed to natural variances rather than a consistent performance trend. The standard deviations and confidence intervals reinforce that the timings are closely clustered, suggesting that any observed differences might not be statistically significant. This data indicates that using provably secure parameters for signature creation with PKCS\#1 v1.5 does not introduce an overhead, and their performance is essentially on par with standard parameters.

\subsection{Signature Creation Results (ANSI X9.31 rDSA)}

Benchmarking across all key sizes for ANSI X9.31 rDSA signature creation reveals a consistent trend. With standard parameters using two primes, SHA-256 mean times range from 30.13104 ms to 30.41202 ms. For provably secure parameters with two primes, the mean times across different hash functions vary from 30.08570 ms to 30.50728 ms, demonstrating a performance variation of up to 1.39\%. With three primes, standard parameters yield mean times from 30.10680 ms to 30.41080 ms, while for provably secure parameters, the range is from 30.10680 ms to 30.86716 ms, indicating a potential variation of up to 1.49\% across all key sizes.


These results align with findings from PKCS\#1 v1.5, in that the difference between standard and provably secure parameters is negligible. The small fluctuations in performance times are likely inherent to computational processes and do not signify any significant statistical difference. Therefore it can be inferred that instantiation of ANSI X9.31 rDSA (for signature creation) with provably secure parameters does not introduce an overhead, and rather maintains effectiveness in line with standard parameters.

\subsection{Signature Creation Results (ISO/IEC 9796-2:2010 Signature Scheme 1)}

The ISO/IEC 9796-2:2010 Signature Scheme 1 exhibits a similar performance profile in its benchmarking for signature creation across all key sizes. Standard parameters using two primes for SHA-256 show mean times between 28.57240 ms and 28.72859 ms. For provably secure parameters with two primes, the mean times across different hash functions vary from 28.48672 ms to 28.74465 ms, demonstrating a performance variation of up to 1.39\%. With three primes, standard parameters yield mean times from 28.60973 ms to 28.73497 ms, while for provably secure parameters, the range is from 28.54507 ms to 28.80356 ms, indicating a potential variation of up to 1.49\% across all key sizes.


Like the former two schemes ISO/IEC 9796-2:2010 Signature Scheme 1 demonstrates that the use of provably secure parameters for signature creation yields only negligible differences in performance when compared to standard parameters.  The small fluctuations in performance times are likely inherent to computational processes and do not signify any significant statistical difference. Therefore it can be inferred that instantiation of ISO-IEC 9796-2 Scheme 1 (for signature creation) with provably secure parameters does not introduce an overhead, and rather maintains effectiveness in line with standard parameters.


\section{Signature Verification Results}

\subsection{Signature Verification Results (PKCS\#1 v1.5)}
\begin{figure}[H]
    \centering % Center the images
     \caption{Instantiation of PKCS\#1 v1.5 with standard vs provably secure parameters (1024-bit Key Size) for signature verification}
    % First image in a minipage
    \begin{minipage}{\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_verify_1024bit_table1_1.png}} 
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_verify_1024bit_table2_1.png}}
    \end{minipage}
       \label{pkcs_verify_1024bit_table}
  \end{figure}
  
\begin{figure}[H]
    \centering % Center the images
     \caption{Instantiation of PKCS\#1 v1.5 with standard vs provably secure parameters (2048-bit Key Size) for signature verification}
    % First image in a minipage
    \begin{minipage}{\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_verify_2048bit_table1_1.png}} 
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_verify_2048bit_table2_1.png}}
    \end{minipage}
           \label{pkcs_verify_2048bit_table}
  \end{figure}
  
\begin{figure}[H]
    \centering % Center the images
     \caption{Instantiation of PKCS\#1 v1.5 with standard vs provably secure parameters (3072-bit Key Size) for signature verification}
    % First image in a minipage
    \begin{minipage}{\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_verify_3072bit_table1_1.png}} 
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_verify_3072bit_table2_1.png}}
    \end{minipage}
          \label{pkcs_verify_3072bit_table}
\end{figure}

\begin{figure}[H]
    \centering % Center the images
     \caption{Instantiation of PKCS\#1 v1.5 with standard vs provably secure parameters (4096-bit Key Size) for signature verification}
    % First image in a minipage
    \begin{minipage}{\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_verify_4096bit_table1_1.png}} 
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_verify_4096bit_table2_1.png}}
    \end{minipage}
         \label{pkcs_verify_4096bit_table}
\end{figure}

\begin{figure}[H]
    \centering % Center the images
     \caption{Instantiation of PKCS\#1 v1.5 with standard vs provably secure parameters (5120-bit Key Size) for signature verification}
    % First image in a minipage
    \begin{minipage}{\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_verify_5120bit_table1_1.png}} 
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_verify_5120bit_table2_1.png}}
    \end{minipage}
       \label{pkcs_verify_5120bit_table}
\end{figure}

\begin{figure}[H]
    \centering % Center the images
     \caption{Instantiation of PKCS\#1 v1.5 with standard vs provably secure parameters (6144-bit Key Size) for signature verification}
    % First image in a minipage
    \begin{minipage}{\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_verify_6144bit_table1_1.png}} 
        \fbox{\includegraphics[width=\textwidth]{main_pictures/pkcs/pkcs_verify_6144bit_table2_1.png}}
    \end{minipage}
         \label{pkcs_verify_6144bit_table}
\end{figure}




\subsection*{Standard Parameters}
\begin{itemize}
\item 2 Primes: Mean verification time with SHA-256 ranges from 0.50316 ms (1024-bit) to 77.02268 ms (6144-bit).
\item 3 Primes: Mean verification time with SHA-256 ranges from 0.49373 ms (1024-bit) to 77.10924 ms (6144-bit).
\end{itemize}

\subsection*{Provable Parameters (2 Primes)}

\begin{itemize}
\item SHA-512 with MGF1: Shows improvements in verification time compared to standard parameter instantiations with 2 Primes, with reductions ranging from 73.13\% to 74.62\%. This hash function also consistently exhibits lower variance compared to other provably secure parameter instantiations
\item SHAKE-256: Demonstrates improvements in verification time compared to standard parameters with 2 Primes, with reductions ranging from 69.94\% to 75.07\%. It generally shows lower variance across the various key sizes.
\item SHA-256 with MGF1: Demonstrates improvement in verification time compared to standard parameters with 2 Primes with reductions ranging from 69.22\% to 72.81\%. While it shows moderately low variance, it is generally higher compared to SHA-512 with MGF1 and SHAKE-256.
\item SHAKE-128: Offers smallest in verification time compared to standard parameters with 2 Primes from 64.78\% to 68.37\%. However, it consistently presents the highest variance among the provably secure parameter instantiations.
\end{itemize}

\subsection*{Provable Parameters (3 Primes)}
\begin{itemize}
\item SHA-512 with MGF1: Provides improvements in verification time compared to standard parameters with 3 Primes, with reductions ranging from 73.29\% to 74.47\%. Similar to its performance with 2 Primes, it exhibits low variance across key sizes.
\item SHAKE-256: Shows improvements in verification time compared to standard parameters with 3 Primes, with reductions ranging from 71.36\% to 75.10\%. It consistently demonstrates one of the lowest variances.
\item SHA-256 with MGF1: Shows improvement in verification time compared to standard parameters instantiations with 3 Primes, ranging from 70.20\% to 73.11\%. While its variance is moderately low, it is higher compared to SHA-512 with MGF1 and SHAKE-256.
\item SHAKE-128: Provides the smallest improvements in verification time compared to standard parameters with 3 Primes, ranging from 61.94\% to 65.62\%, and consistently shows the highest variance. Notably, when evaluated outside the 3 Prime context, SHAKE-128 shows relatively smaller improvements in verification time compared to its performance with 2 Primes across all key sizes. 
\end{itemize}

\subsection*{Summary}

The analysis revealed a surprising outcome: employing provably secure parameters in the PKCS\#1 v1.5 signature scheme led to significant improvement in verification times as compared to doing the same with standard parameters, challenging initial expectations. This unexpected efficiency gain can be attributed primarily to the relationship between the public exponent e and the hash function's output size in determining the computational efficiency of signature verification. It's established that smaller values of e result in more efficient verification processes, as they require fewer bits and operations for exponentiation. The results were aligned with this.

The key determinant of computational time was found to be the use of smaller 'e' values in provably secure configurations, as opposed to the arbitrary values in standard configurations. The role of the hash function and its output size, while still relevant, was secondary to the choice of 'e'. This effectively balanced any potential negative impact on computational time from employing large hash output sizes ($\lambda / 2$) in provably secure instantiations of the PKCS\#1 scheme. 

Upon isolating the value of e, variations in signature verification time emerged across the range of hash functions used in provably secure instantiations. This was evident even without altering the hash output size, but merely by employing different variable-length hash functions with their varying levels of security and/or construction mechanisms used to achieve the same output size.

For instance, the results revealed a direct correlation between the strength of the hash function and the relative improvements in verification times. Higher-strength hash functions such as SHA-512 with MGF1 and SHAKE-256 not only demonstrated the greatest improvements in verification times but also maintained the lowest variances across key sizes. On the other hand, SHAKE-128, which represents the lowest security level among the variable-length hash functions, consistently demonstrated the highest variance and the least improvement among the provably secure instantiations for all key sizes.

\newpage
\textbf{SHAKE-128 in different prime settings}

For all hash functions used in provably secure instantiations bar SHAKE-128, the improvement in verification times as compared between 2 prime and 3 primes was negligible. However, Interestingly for SHAKE-128, there was a noticeable difference between 2 and 3 prime provably secure instantiations. 3 prime provably secure instantiations were slower in signature verifications and thus lead to smallest improvement in verification times for every key size. The difference in improvement between 2 and 3 primes for SHAKE-128 ranged from 1.79\% to 3.18\%  less improvement for 3 primes compared to 2 primes.


Overall the data indicates there is a significant improvement in verification time from instantiating the PKCS\#1 v1.5 signature scheme using provably secure parameters, with improvements for the full set of key sizes ranging from 64.78\% to 75.07\% in the 2 prime setting and 61.94\% to 75.1\% in the 3 prime setting, contingent upon the security level of the hash function employed for instantiation. Additionally the data indicates that while SHAKE-128 offers improvement in verification times in both 2 and 3 prime settings, the relative efficiency is somewhat reduced in the 3 prime setup for all key sizes.

\subsection{Signature Verification Results (ANSI X9.31 rDSA)}

The analysis of the ANSI X9.31 rDSA scheme corroborates the findings from the PKCS\#1 v1.5 scheme, indicating that provably secure parameters substantially enhance verification times across all key sizes.

\textbf{Hash Function Influence:}  

The Results from ANSI X9.31 rDSA scheme reinforce the secondary impact of the hash function's output size and type on computational time in relation to values of e. Moreover, higher-strength hash functions, demonstrated superior efficiency in verification time reduction with low variance.

\textbf{SHAKE-128 Discrepancy:} 

SHAKE-128, within provably secure instantiations of ANSI X9.31 rDSA, exhibited notable differences between 2-prime and 3-prime settings. The improvement in verification time was less pronounced in 3-prime configurations, with differences ranging from 0.95\% to 8.6\%. Notably, the discrepancy is more pronounced than that observed in the PKCS\#1 v1.5 scheme.

Overall, the analysis of the ANSI X9.31 rDSA scheme demonstrates that employing provably secure parameters leads to significant improvements in verification times, with reductions ranging from 70.1\% to 75.57\% in 2-prime settings and 61.83\% to 75.07\% in 3-prime settings. These gains are influenced by the security level of the hash function employed, with SHAKE-128 presenting unique variations in efficiency between different prime settings.

\newpage
\subsection{Signature Verification Results (ISO/IEC 9796-2:2010 Signature Scheme 1)}

The analysis of the ISO/IEC 9796-2:2010 Signature Scheme 1 corroborates the findings from the latter two schemes indicating that provably secure parameters substantially enhance verification times across all key sizes.

\textbf{Hash Function Influence:}  

The Results from ISO/IEC 9796-2:2010 Signature Scheme 1 again reinforce the secondary impact of the hash function's output size and type on computational time in relation to values of e. Moreover, higher-strength hash functions, demonstrated superior efficiency in verification time reduction with low variance.

\textbf{SHAKE-128 Discrepancy:} 

SHAKE-128, within provably secure instantiations of ISO/IEC 9796-2:2010 Signature Scheme 1 exhibited notable differences between 2-prime and 3-prime settings. Again the improvement in verification time was less pronounced in 3-prime configurations, with differences ranging from 0.23\%  to 10.6\%. 

Overall, the analysis of the ISO/IEC 9796-2:2010 Signature Scheme 1 scheme demonstrates that employing provably secure parameters leads to significant improvements in verification times, with reductions ranging from 69.64\% to 75.26\% in 2-prime settings and 62.14\% to 75.08\% in 3-prime settings. These gains are influenced by the security level of the hash function employed, with SHAKE-128 presenting unique variations in efficiency between different prime settings.



\begin{comment}


\subsection{1024-bit Key Size}
At the 1024-bit key size, a detailed analysis reveals a discernible pattern in the efficiency of signature verification, contrasting standard and provably secure parameters. 

- \textbf{Standard Parameters (2 Primes)}: With SHA-256, the mean verification time is 0.50316 ms. 
- \textbf{Standard Parameters (3 Primes)}: With SHA-256, this time slightly reduces to 0.49373 ms.
- \textbf{Provable Parameters (2 Primes)}: Notable improvements are observed. For SHA-256 with MGF1 (512bit), the mean time is 0.15487 ms, a significant reduction of 69.23\% compared to standard parameters. With SHA-512 with MGF1 (512bit), this improvement is even more pronounced at 74.41\% (0.12868 ms), and for SHAKE-128 (512bit) and SHAKE-256 (512bit), the reductions are 68.36\% (0.15913 ms) and 74.63\% (0.12772 ms), respectively.
- \textbf{Provable Parameters (3 Primes)}: Similar trends are observed. SHA-256 with MGF1 (512bit) reduces the time to 0.14711 ms (70.22\% improvement), SHA-512 with MGF1 (512bit) to 0.12834 ms (74.48\% improvement), SHAKE-128 (512bit) to 0.17171 ms (65.16\% improvement), and SHAKE-256 (512bit) to 0.12660 ms (74.32\% improvement).

These results indicate a consistent advantage of provably secure parameters over standard ones at this key size, with improvements ranging from 65\% to 75\%.

SHA-512 with MGF1 (512bit) is the most efficient with a 74.41\% to 74.48\% improvement. It also has lower variance, suggesting more consistent performance.
SHAKE-256 (512bit) closely follows with a 74.32\% to 74.63\% improvement, demonstrating a similar level of efficiency.
SHA-256 with MGF1 (512bit) shows a lesser improvement of around 69.23\% to 70.22\%.
SHAKE-128 (512bit) has the least improvement, around 65.16\% to 68.36\%, and the highest variance, indicating less consistent performance.

\subsection{2048-bit Key Size}
At this size, the efficiency gains continue but the magnitude varies.

- \textbf{Standard Parameters (2 Primes)}: SHA-256 yields a mean time of 3.21295 ms.
- \textbf{Standard Parameters (3 Primes)}: Slight reduction to 3.19978 ms.
- \textbf{Provable Parameters (2 Primes)}: SHA-256 with MGF1 (1024bit) shows a 72.04\% improvement (0.89793 ms), SHA-512 with MGF1 (1024bit) a 74.63\% improvement (0.81559 ms), SHAKE-128 (1024bit) a 67.79\% improvement (1.03558 ms), and SHAKE-256 (1024bit) a 75.08\% improvement (0.80089 ms).
- \textbf{Provable Parameters (3 Primes)}: SHA-256 with MGF1 (1024bit) records a 72.77\% improvement (0.87115 ms), SHA-512 with MGF1 (1024bit) a 74.47\% improvement (0.81682 ms), SHAKE-128 (1024bit) a 63.95\% improvement (1.15479 ms), and SHAKE-256 (1024bit) a 75.10\% improvement (0.79681 ms).

The provably secure parameters maintain a substantial efficiency advantage, ranging from around 64\% to 75\% reduction in verification time compared to standard parameters.


SHAKE-256 (1024bit) leads with around 75.08\% to 75.10\% improvement, coupled with low variance for consistent results.
SHA-512 with MGF1 (1024bit) is almost as efficient, showing 74.47\% to 74.63\% improvement.
SHA-256 with MGF1 (1024bit) ranges from 72.04\% to 72.77\% improvement.
SHAKE-128 (1024bit), again, is less efficient with a 63.95\% to 67.79\% improvement, coupled with higher variance.


\subsection{3072-bit Key Size}
Here, the trend is similar, but the efficiency gains are slightly less than at smaller key sizes.

- \textbf{Standard Parameters (2 Primes)}: Mean time with SHA-256 is 10.00417 ms.
- \textbf{Standard Parameters (3 Primes)}: Marginally lower at 9.98301 ms.
- \textbf{Provable Parameters (2 Primes)}: SHA-256 with MGF1 (1536bit) shows a 72.61\% improvement (2.74499 ms), SHA-512 with MGF1 (1536bit) a 74.35\% improvement (2.56558 ms), SHAKE-128 (1536bit) a 65.06\% improvement (3.49753 ms), and SHAKE-256 (1536bit) a 74.45\% improvement (2.55588 ms).
- \textbf{Provable Parameters (3 Primes)}: SHA-256 with MGF1 (1536bit) records a 73.06\% improvement (2.69096 ms), SHA-512 with MGF1 (1536bit) a 74.29\% improvement (2.57252 ms), SHAKE-128 (1536bit) a 62.93\% improvement (3.70666 ms),
- \textbf{Provable Parameters (3 Primes)} (continued): For SHAKE-256 (1536bit), the verification time is 2.53819 ms, indicating a 74.57\% improvement over standard parameters. 

The provably secure parameters with both 2 and 3 primes maintain a substantial efficiency advantage, with improvements typically ranging between 62\% to 75\%.

SHAKE-256 (1536bit) and SHA-512 with MGF1 (1536bit) are the top performers, both offering around 74\% improvement with low variance.
SHA-256 with MGF1 (1536bit) provides about 72\% to 73\% improvement.
SHAKE-128 (1536bit) shows a lower efficiency of around 62.93\% to 65.06\% and higher variance.


\subsection{4096-bit Key Size}
As the key size increases further, the pattern of efficiency gains continues, albeit with slightly varied magnitudes.

- \textbf{Standard Parameters (2 Primes)}: SHA-256 shows a mean verification time of 22.96845 ms.
- \textbf{Standard Parameters (3 Primes)}: A slight decrease to 22.89469 ms.
- \textbf{Provable Parameters (2 Primes)}: The times decrease significantly for SHA-256 with MGF1 (2048bit) to 6.29515 ms (72.60\% improvement), SHA-512 with MGF1 (2048bit) to 6.04852 ms (73.67\% improvement), SHAKE-128 (2048bit) to 8.08839 ms (64.75\% improvement), and SHAKE-256 (2048bit) to 6.12497 ms (73.33\% improvement).
- \textbf{Provable Parameters (3 Primes)}: Similarly, SHA-256 with MGF1 (2048bit) reduces to 6.24518 ms (72.76\% improvement), SHA-512 with MGF1 (2048bit) to 6.11577 ms (73.30\% improvement), SHAKE-128 (2048bit) to 8.71372 ms (61.96\% improvement), and SHAKE-256 (2048bit) to 6.02684 ms (73.69\% improvement).

The trend shows that as the key size increases, the efficiency gains of provably secure parameters over standard parameters are maintained, with reductions in verification time generally ranging from 62\% to 74\%.

SHAKE-256 (2048bit) and SHA-512 with MGF1 (2048bit) remain the best, both exceeding 73\% improvement with minimal variance.
SHA-256 with MGF1 (2048bit) provides around 72\% to 73\% efficiency gain.
SHAKE-128 (2048bit) lags behind with a 61.96\% to 64.75\% improvement and increased variance.

\subsection{5120-bit Key Size}
At this key size, the trend of efficiency gains is maintained, though the percentages slightly vary.

- \textbf{Standard Parameters (2 Primes)}: The mean time with SHA-256 is 44.17343 ms.
- \textbf{Standard Parameters (3 Primes)}: A small reduction to 44.16998 ms.
- \textbf{Provable Parameters (2 Primes)}: The efficiency gains remain significant. SHA-256 with MGF1 (2560bit) shows a mean time of 12.00890 ms (72.83\% improvement), SHA-512 with MGF1 (2560bit) 11.82541 ms (73.24\% improvement), SHAKE-128 (2560bit) 15.33917 ms (65.25\% improvement), and SHAKE-256 (2560bit) 13.27861 ms (69.94\% improvement).
- \textbf{Provable Parameters (3 Primes)}: For SHA-256 with MGF1 (2560bit), the mean time is 11.87869 ms (73.13\% improvement), SHA-512 with MGF1 (2560bit) 11.69189 ms (73.52\% improvement), SHAKE-128 (2560bit) 16.18256 ms (63.39\% improvement), and SHAKE-256 (2560bit) 12.65173 ms (71.36\% improvement).

The efficiency gains in this category range roughly between 63\% and 73\% compared to standard parameters, indicating consistent performance advantages for provably secure parameters.

SHA-512 with MGF1 (2560bit) shows the highest efficiency, with around 73\% improvement.
SHAKE-256 (2560bit) closely follows with similar performance.
SHA-256 with MGF1 (2560bit) offers about 72\% to 73\% improvement.
SHAKE-128 (2560bit) is the least efficient, with approximately 63\% to 65\% improvement.

\subsection{6144-bit Key Size}
At the largest tested key size, the efficiency differences between standard and provably secure parameters are as follows:

- \textbf{Standard Parameters (2 Primes)}: SHA-256 has a mean time of 77.02268 ms.
- \textbf{Standard Parameters (3 Primes)}: Slightly higher at 77.10924 ms.
- \textbf{Provable Parameters (2 Primes)}: The mean times reduce considerably. SHA-256 with MGF1 (3072bit) records 21.94449 ms (71.50\% improvement), SHA-512 with MGF1 (3072bit) 20.69202 ms (73.15\% improvement), SHAKE-128 (3072bit) 25.09349 ms (67.42\% improvement), and SHAKE-256 (3072bit) 23.15666 ms (69.95\% improvement).
- \textbf{Provable Parameters (3 Primes)}: For SHA-256 with MGF1 (3072bit), the mean time is 21.41952 ms, a 72.18\% improvement over the standard parameters. In the case of SHA-512 with MGF1 (3072bit), the time further decreases to 20.28377 ms, marking a 73.70\% improvement. With SHAKE-128 (3072bit), the efficiency is increased to 26.50817 ms (65.57\% improvement), and SHAKE-256 (3072bit) shows a 71.57\% improvement with a mean time of 21.98948 ms.

SHA-512 with MGF1 (3072bit) stands out with up to 73.70\% improvement, indicating strong efficiency even at higher key sizes.
SHAKE-256 (3072bit) maintains a consistent efficiency above 69\%.
SHA-256 with MGF1 (3072bit) and SHAKE-128 (3072bit) show comparatively lower improvements, with SHAKE-128 demonstrating the least efficiency.



\textbf{Overall Trend and Implications:}
Across all key sizes, we observe a consistent pattern where provably secure parameters consistently outperform their standard counterparts in terms of efficiency. 

\end{comment}


\chapter{Conclusion}
To be done.



%%%% ADD YOUR BIBLIOGRAPHY HERE
\newpage

\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography
\label{endpage}
\end{document}

\end{article}
